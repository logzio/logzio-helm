# Default values for opentelemetry-collector.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# conditionally controll the deployment of this chart by a parent chart 
enabled: true

nameOverride: ""
fullnameOverride: ""

# Valid values for metrics collection are "daemonset","standalone". Default is "daemonset"
mode: "daemonset"

# Specify which namespace should be used to deploy the resources into
namespaceOverride: ""

 
secrets:
  # When true, the logzio secret will be created and managed by this Chart. If you're managing the logzio secrets by yourself, set to false, note that in order for the default configuration to work proprly you need to create the following env variables: ENV_ID LOGZIO_REGION LOGZIO_METRICS_TOKEN
  enabled: true
  name: logzio-metric-collector-secrets
  # environment indentifier attribute that will be added to all metrics
  env_id: "my_env"
  # Secret with your Logz.io metrics shipping token
  logzioMetricsToken: "<<METRICS-SHIPPING-TOKEN>>"
  # Secret with your Logz.io region code - https://docs.logz.io/docs/user-guide/admin/hosting-regions/account-region/
  logzioRegion: "us" 
  # Secret with your custom endpoint, for example: http://endpoint:8050. Overrides secrets.logzioRegion listener adress
  customEndpoint: ""

configMap:
  # Specifies whether a configMap should be created (true by default)
  create: true

# OpenTelemetry Collector base configuration
config:
  receivers:
    k8s_cluster:
      auth_type: serviceAccount
      collection_interval: 30s
      metadata_collection_interval: 1m
      node_conditions_to_report:
        - Ready
        - MemoryPressure
        - NetworkUnavailable
        - DiskPressure
        - PIDPressure
      allocatable_types_to_report:
        - cpu
        - memory
        - storage
        - ephemeral-storage
      resource_attributes:
        k8s.pod.name:
          enabled: true
        k8s.deployment.name:
          enabled: true
        k8s.namespace.name:
          enabled: true
        k8s.node.name:
          enabled: true
        k8s.statefulset.name:
          enabled: true
        k8s.replicaset.name:
          enabled: true
        k8s.daemonset.name:
          enabled: true
        k8s.cronjob.name:
          enabled: true
        k8s.job.name:
          enabled: true
        k8s.pod.uid:
          enabled: true
    kubeletstats:
      collection_interval: 30s
      auth_type: "serviceAccount"
      endpoint: "${env:KUBE_NODE_NAME}:10250"
      insecure_skip_verify: true
    prometheus/collector:
      config:
        global:
          scrape_interval: 30s
          scrape_timeout: 30s
        scrape_configs:
        # Job to collect opentelemetry collector metrics
        - job_name: 'collector-metrics'
          scrape_interval: 15s
          static_configs:
          - targets: [ "0.0.0.0:8888" ] 
          metric_relabel_configs: []
  exporters:
    logging:
      loglevel: info
    prometheusremotewrite/applications:
      timeout: 30s
      endpoint: ${LISTENER_URL}
      external_labels:
        p8s_logzio_name: ${P8S_LOGZIO_NAME}
      headers:
        Authorization: "Bearer ${METRICS_TOKEN}"
        user-agent: "{{ .Chart.Name }}-{{ .Chart.Version }}-helm"
      resource_to_telemetry_conversion:
        enabled: true
    prometheusremotewrite/infrastructure:
      timeout: 30s
      endpoint: ${LISTENER_URL}
      external_labels:
        p8s_logzio_name: ${P8S_LOGZIO_NAME}
      headers:
        Authorization: "Bearer ${METRICS_TOKEN}"
        user-agent: "{{ .Chart.Name }}-{{ .Chart.Version }}-helm"    
      resource_to_telemetry_conversion:
        enabled: true
  extensions:
    health_check: {}
  processors:
    k8sattributes:
      extract:
        metadata:
        - k8s.pod.name
        - k8s.deployment.name
        - k8s.namespace.name
        - k8s.node.name
        - k8s.statefulset.name
        - k8s.replicaset.name
        - k8s.daemonset.name
        - k8s.cronjob.name
        - k8s.job.name
        - k8s.pod.uid
        - k8s.pod.start_time
        - k8s.pod.ip
      filter:
        node_from_env_var: KUBE_NODE_NAME
      passthrough: false
      pod_association:
        - sources:
            - from: resource_attribute
              name: k8s.pod.ip
        - sources:
            - from: resource_attribute
              name: k8s.pod.uid
        - sources:
            - from: connection
    filter/kubernetes360:
      metrics:
        datapoint:
          - 'IsMatch(metric.name, "(${K8S_360_METRICS})") == true and attributes["logzio_app"] != "kubernetes360"'        
    batch: {}
    attributes/env_id:
      actions:
        - key: env_id
          value: ${ENV_ID}
          action: insert
        - key: logzio_agent_version
          value: "{{ .Chart.Version }}"
          action: insert
  service:
    extensions:
      - health_check
    telemetry:
      logs:
        level: "debug"  


# Configuration for OpenTelemetry Collector DaemonSet, enabled by default
daemonsetCollector:

  containerLogs:
    enabled: false
  
  # prevent collector daemonset deployment on fargate nodes
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: eks.amazonaws.com/compute-type
                operator: DoesNotExist

  resources:
    limits:
      memory: 250Mi
    requests:
      cpu: 150m

  podLabels:  {}

  podAnnotations: {}
  
  # DaemonSet Configuration override that will be merged into the daemonset default config
  configOverride:
    receivers: 
      prometheus/applications:
        config:
          global:
            scrape_interval: 30s
            scrape_timeout: 30s
          scrape_configs:
          - job_name: applications
            honor_timestamps: true
            honor_labels: true
            metrics_path: /metrics
            scheme: http
            kubernetes_sd_configs:
            - role: pod
              selectors:
              # only scrape data from pods running on the same node as the collector
              - role: pod
                field: "spec.nodeName=$KUBE_NODE_NAME"
            relabel_configs:
              - action: keep
                regex: true|"true"
                source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
              - action: replace
                regex: (https?)
                source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]
                target_label: __scheme__
              - action: replace
                regex: (.+)
                source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
                target_label: __metrics_path__
              - action: replace
                regex: (.+?)(?::\d+)?;(\d+)
                replacement: $$1:$$2
                source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
                target_label: __address__
              - action: labelmap
                regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)
                replacement: __param_$$1
              - action: labelmap
                regex: __meta_kubernetes_pod_label_(.+)
              - action: replace
                source_labels: [__meta_kubernetes_namespace]
                target_label: namespace
              - action: replace
                source_labels: [__meta_kubernetes_pod_name]
                target_label: pod
              - source_labels: [__meta_kubernetes_pod_node_name]
                action: replace
                target_label: kubernetes_node          
            metric_relabel_configs: []    
      prometheus/infrastructure:
        config:
          global:
            scrape_interval: 30s
            scrape_timeout: 30s
          scrape_configs:
          - job_name: windows-metrics
            honor_timestamps: true
            honor_labels: true
            metrics_path: /metrics
            scheme: http
            kubernetes_sd_configs:
            - role: pod
              selectors:
              # only scrape data from pods running on the same node as collector
              - role: pod
                field: "spec.nodeName=$KUBE_NODE_NAME"
            relabel_configs:
              - source_labels: [__meta_kubernetes_pod_annotation_prometheus_windows_io_scrape]
                action: keep
                regex: true|"true"
            metric_relabel_configs: []
          # Job to collect metrics from applications running on pods
          - job_name: kubernetes-service-endpoints
            honor_timestamps: true
            honor_labels: true
            metrics_path: /metrics
            scheme: http
            kubernetes_sd_configs:
            - role: endpoints
              selectors:
              # only scrape data from pods running on the same node as collector
              - role: pod
                field: "spec.nodeName=$KUBE_NODE_NAME"
            relabel_configs:
              # Adding a dummy job label to enable filtering for duplicate metrics using daemonset collector
              # "job" label is only added in the prometheusremotewrite and cannot be added before
              - action: replace
                replacement: kubernetes-service-endpoints
                target_label: job_dummy
              - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
                action: keep
                regex: true|"true"
              - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
                action: replace
                target_label: __scheme__
                regex: (https?)
              - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
                action: replace
                target_label: __metrics_path__
                regex: (.+)
              - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
                action: replace
                target_label: __address__
                regex: ([^:]+)(?::\d+)?;(\d+)
                replacement: $$1:$$2
              - action: labelmap
                regex: __meta_kubernetes_service_label_(.+)
              - source_labels: [__meta_kubernetes_namespace]
                action: replace
                target_label: namespace
              - source_labels: [__meta_kubernetes_service_name]
                action: replace
                target_label: service
              - source_labels: [__meta_kubernetes_pod_name]
                action: replace
                target_label: pod
              - source_labels: [__meta_kubernetes_pod_node_name]
                action: replace
                target_label: kubernetes_node
              - source_labels: [__meta_kubernetes_service_annotation_logz_io_app]
                action: replace
                target_label: logzio_app                 
            metric_relabel_configs: []
    processors:
      resourcedetection/all:
        detectors: [ec2, azure, gcp]
      filter/kubernetes360:
        metrics:
          datapoint:
            - 'IsMatch(metric.name, "(${K8S_360_METRICS})") == true and attributes["logzio_app"] != "kubernetes360"'
            # Workaround for an issue where metrics are scraped multiple times
            - 'attributes["job_dummy"] == "kubernetes-service-endpoints" and attributes["kubernetes_node"] == nil'
      # Removes label needed for duplicate metrics checks
      attributes/remove_job_dummy:
        actions:
          - key: job_dummy
            action: delete    
    service:
      extensions:
        - health_check
      pipelines:
        metrics/infrastructure:
          exporters:
            - prometheusremotewrite/infrastructure
          processors:
            - attributes/env_id
            - batch
            # - filter/kubernetes360
            - k8sattributes
            # workaround for duplicate metrics sent from kubernetes-service-endpoints job
            - attributes/remove_job_dummy
          receivers:
            - prometheus/infrastructure
            - kubeletstats
            - k8s_cluster
            - prometheus/collector
                      

# Configuration for standalone OpenTelemetry Collector deployment
standaloneCollector:
  replicas: 1

  containerLogs:
    enabled: false

  resources:
    limits:
      memory: 512Mi
    requests:
      cpu: 200m
      
  podLabels:  {}

  podAnnotations: {}
  
  # Standalone collector configuration override that will be merged into the collector default config
  configOverride: 
    receivers:
      prometheus/applications:
        config:
          global:
            scrape_interval: 60s
            scrape_timeout: 60s
          scrape_configs:
          - job_name: applications
            honor_timestamps: true
            honor_labels: true
            metrics_path: /metrics
            scheme: http
            kubernetes_sd_configs:
            - role: pod
            relabel_configs:
              - action: keep
                regex: true|"true"
                source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
              - action: replace
                regex: (https?)
                source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]
                target_label: __scheme__
              - action: replace
                regex: (.+)
                source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
                target_label: __metrics_path__
              - action: replace
                regex: (.+?)(?::\d+)?;(\d+)
                replacement: $$1:$$2
                source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
                target_label: __address__
              - action: labelmap
                regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)
                replacement: __param_$$1
              - action: labelmap
                regex: __meta_kubernetes_pod_label_(.+)
              - action: replace
                source_labels: [__meta_kubernetes_namespace]
                target_label: namespace
              - action: replace
                source_labels: [__meta_kubernetes_pod_name]
                target_label: pod
              - source_labels: [__meta_kubernetes_pod_node_name]
                action: replace
                target_label: kubernetes_node          
            metric_relabel_configs: []
      prometheus/infrastructure:
        config:
          global:
            scrape_interval: 30s
            scrape_timeout: 30s
          scrape_configs:
          - job_name: windows-metrics
            honor_timestamps: true
            honor_labels: true
            metrics_path: /metrics
            scheme: http
            kubernetes_sd_configs:
            - role: pod
            relabel_configs:
              - source_labels: [__meta_kubernetes_pod_annotation_prometheus_windows_io_scrape]
                action: keep
                regex: true|"true"
            metric_relabel_configs: []
          - job_name: kubernetes-service-endpoints
            honor_timestamps: true
            honor_labels: true
            metrics_path: /metrics
            scheme: http
            kubernetes_sd_configs:
            - role: endpoints
            relabel_configs:
              - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
                action: keep
                regex: true|"true"
              - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
                action: replace
                target_label: __scheme__
                regex: (https?)
              - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
                action: replace
                target_label: __metrics_path__
                regex: (.+)
              - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
                action: replace
                target_label: __address__
                regex: ([^:]+)(?::\d+)?;(\d+)
                replacement: $$1:$$2
              - action: labelmap
                regex: __meta_kubernetes_service_label_(.+)
              - source_labels: [__meta_kubernetes_namespace]
                action: replace
                target_label: namespace
              - source_labels: [__meta_kubernetes_service_name]
                action: replace
                target_label: service
              - source_labels: [__meta_kubernetes_pod_name]
                action: replace
                target_label: pod
              - source_labels: [__meta_kubernetes_pod_node_name]
                action: replace
                target_label: kubernetes_node
              - source_labels: [__meta_kubernetes_service_annotation_logz_io_app]
                action: replace
                target_label: logzio_app                 
            metric_relabel_configs: []
    service:
      extensions:
        - health_check
      pipelines:
        metrics/infrastructure:
          exporters:
            - prometheusremotewrite/infrastructure
          processors:
            - attributes/env_id
            - batch
            # - filter/kubernetes360
            - k8sattributes
          receivers:
            - prometheus/infrastructure
            - kubeletstats
            - k8s_cluster
            - prometheus/collector
          
# OpenTelemetry Collector image
image:
  # If you want to use the core image `otel/opentelemetry-collector`, you also need to change `command.name` value to `otelcol`.
  repository: otel/opentelemetry-collector-contrib
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""
  # When digest is set to a non-empty value, images will be pulled by digest (regardless of tag value).
  digest: ""
imagePullSecrets: []

# OpenTelemetry Collector executable
command:
  name: otelcol-contrib
  extraArgs: []

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

clusterRole:
  # Specifies whether a clusterRole should be created
  # Some presets also trigger the creation of a cluster role and cluster role binding.
  # If using one of those presets, this field is no-op.
  create: true
  # Annotations to add to the clusterRole
  # Can be used in combination with presets that create a cluster role.
  annotations: {}
  # The name of the clusterRole to use.
  # If not set a name is generated using the fullname template
  # Can be used in combination with presets that create a cluster role.
  name: ""
  # A set of rules as documented here : https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  # Can be used in combination with presets that create a cluster role to add additional rules.
  rules: []
  # - apiGroups:
  #   - ''
  #   resources:
  #   - 'pods'
  #   - 'nodes'
  #   verbs:
  #   - 'get'
  #   - 'list'
  #   - 'watch'

  clusterRoleBinding:
    # Annotations to add to the clusterRoleBinding
    # Can be used in combination with presets that create a cluster role binding.
    annotations: {}
    # The name of the clusterRoleBinding to use.
    # If not set a name is generated using the fullname template
    # Can be used in combination with presets that create a cluster role binding.
    name: ""

podSecurityContext: {}
securityContext: {}

nodeSelector: {}
tolerations: []
# Set affinity rules for the scheduler to determine where all DaemonSet pods can be placed.
affinity: {}
# Allows for pod scheduler prioritisation
priorityClassName: ""

extraEnvs: []
extraEnvsFrom: []
extraVolumes: []
extraVolumeMounts: []

ports:
  otlp:
    enabled: true
    containerPort: 4317
    servicePort: 4317
    hostPort: 4317
    protocol: TCP
    # nodePort: 30317
    appProtocol: grpc
  otlp-http:
    enabled: true
    containerPort: 4318
    servicePort: 4318
    hostPort: 4318
    protocol: TCP
  jaeger-compact:
    enabled: true
    containerPort: 6831
    servicePort: 6831
    hostPort: 6831
    protocol: UDP
  jaeger-thrift:
    enabled: true
    containerPort: 14268
    servicePort: 14268
    hostPort: 14268
    protocol: TCP
  jaeger-grpc:
    enabled: true
    containerPort: 14250
    servicePort: 14250
    hostPort: 14250
    protocol: TCP
  zipkin:
    enabled: true
    containerPort: 9411
    servicePort: 9411
    hostPort: 9411
    protocol: TCP
  metrics:
    enabled: true
    containerPort: 8888
    servicePort: 8888
    protocol: TCP

resources:
  limits:
    cpu: 250m
    memory: 512Mi

podAnnotations: {}

podLabels: {}

# Common labels to add to all otel-collector resources. Evaluated as a template.
additionalLabels: {}
#  app.kubernetes.io/part-of: my-app

# Host networking requested for this pod. Use the host's network namespace.
hostNetwork: false

# Adding entries to Pod /etc/hosts with HostAliases
# https://kubernetes.io/docs/tasks/network/customize-hosts-file-for-pods/
hostAliases: []
  # - ip: "1.2.3.4"
  #   hostnames:
  #     - "my.host.com"

# Pod DNS policy ClusterFirst, ClusterFirstWithHostNet, None, Default, None
dnsPolicy: ""

# Custom DNS config. Required when DNS policy is None.
dnsConfig: {}


annotations: {}

# List of extra sidecars to add
extraContainers: []
# extraContainers:
#   - name: test
#     command:
#       - cp
#     args:
#       - /bin/sleep
#       - /test/sleep
#     image: busybox:latest
#     volumeMounts:
#       - name: test
#         mountPath: /test

# List of init container specs, e.g. for copying a binary to be executed as a lifecycle hook.
# Another usage of init containers is e.g. initializing filesystem permissions to the OTLP Collector user `10001` in case you are using persistence and the volume is producing a permission denied error for the OTLP Collector container.
initContainers: []
# initContainers:
#   - name: test
#     image: busybox:latest
#     command:
#       - cp
#     args:
#       - /bin/sleep
#       - /test/sleep
#     volumeMounts:
#       - name: test
#         mountPath: /test
#  - name: init-fs
#    image: busybox:latest
#    command:
#      - sh
#      - '-c'
#      - 'chown -R 10001: /var/lib/storage/otc' # use the path given as per `extensions.file_storage.directory` & `extraVolumeMounts[x].mountPath`
#    volumeMounts:
#      - name: opentelemetry-collector-data # use the name of the volume used for persistence
#        mountPath: /var/lib/storage/otc # use the path given as per `extensions.file_storage.directory` & `extraVolumeMounts[x].mountPath`

# Pod lifecycle policies.
lifecycleHooks: {}
# lifecycleHooks:
#   preStop:
#     exec:
#       command:
#       - /test/sleep
#       - "5"

# liveness probe configuration
# Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
##
livenessProbe:
  # Number of seconds after the container has started before startup, liveness or readiness probes are initiated.
  # initialDelaySeconds: 1
  # How often in seconds to perform the probe.
  # periodSeconds: 10
  # Number of seconds after which the probe times out.
  # timeoutSeconds: 1
  # Minimum consecutive failures for the probe to be considered failed after having succeeded.
  # failureThreshold: 1
  # Duration in seconds the pod needs to terminate gracefully upon probe failure.
  # terminationGracePeriodSeconds: 10
  httpGet:
    port: 13133
    path: /

# readiness probe configuration
# Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
##
readinessProbe:
  # Number of seconds after the container has started before startup, liveness or readiness probes are initiated.
  # initialDelaySeconds: 1
  # How often (in seconds) to perform the probe.
  # periodSeconds: 10
  # Number of seconds after which the probe times out.
  # timeoutSeconds: 1
  # Minimum consecutive successes for the probe to be considered successful after having failed.
  # successThreshold: 1
  # Minimum consecutive failures for the probe to be considered failed after having succeeded.
  # failureThreshold: 1
  httpGet:
    port: 13133
    path: /

service:
  # Enable the creation of a Service.
  enabled: true

  type: ClusterIP
  # type: LoadBalancer
  # loadBalancerIP: 1.2.3.4
  # loadBalancerSourceRanges: []

  # By default, Service of type 'LoadBalancer' will be created setting 'externalTrafficPolicy: Cluster'
  # unless other value is explicitly set.
  # Possible values are Cluster or Local (https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip)
  # externalTrafficPolicy: Cluster

  annotations: {}

  # By default, Service will be created setting 'internalTrafficPolicy: Local' on mode = daemonset
  # unless other value is explicitly set.
  # Setting 'internalTrafficPolicy: Cluster' on a daemonset is not recommended
  # internalTrafficPolicy: Cluster

ingress:
  enabled: false
  # annotations: {}
  # ingressClassName: nginx
  # hosts:
  #   - host: collector.example.com
  #     paths:
  #       - path: /
  #         pathType: Prefix
  #         port: 4318
  # tls:
  #   - secretName: collector-tls
  #     hosts:
  #       - collector.example.com

  # Additional ingresses - only created if ingress.enabled is true
  # Useful for when differently annotated ingress services are required
  # Each additional ingress needs key "name" set to something unique
  additionalIngresses: []
  # - name: cloudwatch
  #   ingressClassName: nginx
  #   annotations: {}
  #   hosts:
  #     - host: collector.example.com
  #       paths:
  #         - path: /
  #           pathType: Prefix
  #           port: 4318
  #   tls:
  #     - secretName: collector-tls
  #       hosts:
  #         - collector.example.com

podMonitor:
  # The pod monitor by default scrapes the metrics port.
  # The metrics port needs to be enabled as well.
  enabled: false
  metricsEndpoints:
    - port: metrics
      # interval: 15s

  # additional labels for the PodMonitor
  extraLabels: {}
  #   release: kube-prometheus-stack

rollout:
  rollingUpdate: {}
  # When 'mode: daemonset', maxSurge cannot be used when hostPort is set for any of the ports
  # maxSurge: 25%
  # maxUnavailable: 0
  strategy: RollingUpdate

networkPolicy:
  enabled: false

  # Annotations to add to the NetworkPolicy
  annotations: {}

  # Configure the 'from' clause of the NetworkPolicy.
  # By default this will restrict traffic to ports enabled for the Collector. If
  # you wish to further restrict traffic to other hosts or specific namespaces,
  # see the standard NetworkPolicy 'spec.ingress.from' definition for more info:
  # https://kubernetes.io/docs/reference/kubernetes-api/policy-resources/network-policy-v1/
  allowIngressFrom: []
  # # Allow traffic from any pod in any namespace, but not external hosts
  # - namespaceSelector: {}
  # # Allow external access from a specific cidr block
  # - ipBlock:
  #     cidr: 192.168.1.64/32
  # # Allow access from pods in specific namespaces
  # - namespaceSelector:
  #     matchExpressions:
  #       - key: kubernetes.io/metadata.name
  #         operator: In
  #         values:
  #           - "cats"
  #           - "dogs"

  # Add additional ingress rules to specific ports
  # Useful to allow external hosts/services to access specific ports
  # An example is allowing an external prometheus server to scrape metrics
  #
  # See the standard NetworkPolicy 'spec.ingress' definition for more info:
  # https://kubernetes.io/docs/reference/kubernetes-api/policy-resources/network-policy-v1/
  extraIngressRules: []
  # - ports:
  #   - port: metrics
  #     protocol: TCP
  #   from:
  #     - ipBlock:
  #         cidr: 192.168.1.64/32

  # Restrict egress traffic from the OpenTelemetry collector pod
  # See the standard NetworkPolicy 'spec.egress' definition for more info:
  # https://kubernetes.io/docs/reference/kubernetes-api/policy-resources/network-policy-v1/
  egressRules: []
  #  - to:
  #      - namespaceSelector: {}
  #      - ipBlock:
  #          cidr: 192.168.10.10/24
  #    ports:
  #      - port: 1234
  #        protocol: TCP

# When enabled, the chart will set the GOMEMLIMIT env var to 80% of the configured
# resources.limits.memory
# If no resources.limits.memory are defined enabling does nothing.
# In a future release this setting will be enabled by default.
# See https://github.com/open-telemetry/opentelemetry-helm-charts/issues/891
# for more details.
useGOMEMLIMIT: false
