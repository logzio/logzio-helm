enabled: true
# Fluentd image in daemonset
image: logzio/logzio-fluentd
# Fluentd image tag
imageTag: 1.5.3

# Fluentd image in daemonset for windows nodes
windowsImage: logzio/fluentd-windows
# Fluentd image tag for windows nodes
windowsImageTag: 0.0.3

# Overrides the Chart name for resources
nameOverride: ""
# Overrides the full name of the resources
fullnameOverride: ""

# API versions for the resources
apiVersions:
  # Daemonset API version
  daemonset: apps/v1
  # Service Account API version
  serviceAccount: v1
  # Cluster Role API version
  clusterRole: rbac.authorization.k8s.io/v1
  # Cluster Role Binding API version
  clusterRoleBinding: rbac.authorization.k8s.io/v1
  # Configmap API version
  configmap: v1
  # Secret API version
  secret: v1

# Value for k8s-app label
k8sApp: fluentd-logzio

# Specifies whether the Chart should be compatible to a RBAC cluster
isRBAC: true
# Specifies whether to run the Damonset with priviliged security context
isPrivileged: false

serviceAccount:
  # Override the name of the service account
  name: ""

fargateLogRouter:
  # Boolen to decide if to configure fargate log router
  enabled: false

# Add to your logs field env_id with identification of the environment you're shipping logs from, usually your k8s cluster name
env_id: "my_environment"

daemonset:
  # Security context for the pod level
  podSecurityContext: {}
  # Security context for the container level
  securityContext: {}
  # Security context for the init container
  initContainerSecurityContext: {}
  # Set tolerations for all DaemonSet pods
  tolerations:
  - key: node-role.kubernetes.io/master
    effect: NoSchedule
  # Set nodeSelector for all DaemonSet pods
  nodeSelector:
    os: linux
  # Set affinity rules for the scheduler to determine where all DaemonSet pods can be placed.
  # The following configuration prevent node exporter deamonset deploymment on fargate nodes
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: eks.amazonaws.com/compute-type
                operator: DoesNotExist

  # Controls whether Fluentd system messages will be enabled
  fluentdSystemdConf: disable
  # Controls the launch of a prometheus plugin that monitors Fluentd
  fluentdPrometheusConf: false
  # Use if you wish to send logs from specific k8s namespaces, space delimited.
  # Should be in the following format:
  # kubernetes.var.log.containers.**_<<NAMESPACE-TO-INCLUDE>>_** kubernetes.var.log.containers.**_<<ANOTHER-NAMESPACE>>_**.
  includeNamespace: ""
  # Enables to validate SSL certificates
  kubernetesVerifySsl: true
  # Match Fluentd's format for kube-apiserver audit logs.
  # Set to audit-json if your audit logs are in json format
  auditLogFormat: audit
  # Container runtime interface of the cluster. Used to determine which configuration to use when concatenating partial logs.
  # Valid options are: docker, containerd
  cri: containerd
  # The interval of refreshing the list of watched log files.
  LogFileRefreshInterval: 60s
  # Specifies which plugin to use as the backend
  logzioBufferType: file
  # Path of the buffer
  logzioBufferPath: /var/log/fluentd-buffers/stackdriver.buffer
  # Controls the behavior when the queue becomes full. Can be one of: throw_exception, block, drop_oldest_chunk
  logzioOverflowAction: block
  # Maximum size of a chunk allowed
  logzioChunkLimitSize: 2M
  # Maximum length of the output queue
  logzioQueueLimitLength: 6
  # Interval, in seconds, to wait before invoking the next buffer flush
  logzioFlushInterval: 5s
  # Maximum interval, in seconds, to wait between retries
  logzioRetryMaxInterval: 30
  # If true, plugin will retry flushing forever
  logzioRetryForever: true
  # Number of threads to flush the buffer
  logzioFlushThreadCount: 2
  # The log level for the Fluentd logs
  logzioLogLevel: info
  # Path to fluentd logs file, to exclude them from the logs that Fluent tails
  excludeFluentdPath: "/var/log/containers/*fluentd*.log"
  # A comma-seperated list (no spaces), of more paths to exclude from the Fluentd source that tails containers logs.
  # For example - /path/one.log,/path/two.log
  extraExclude: ""
  # Path for containers logs
  containersPath: "/var/log/containers/*.log"
  # Path for containers logs pos file
  posFile: "/var/log/fluentd-containers.log.pos"
  # Set log type for the logs
  logType: "k8s"
  # If needed, more env vars can be added with this field
  extraEnv: []
  # Allows you to set the resources for Fluentd Daemonset.
  resources:
    requests:
      cpu: 200m
      memory: 500Mi
  # If needed, more volume mounts can be added with this field
  extraVolumeMounts: []
  # Termination period (in seconds) to wait before killing Fluentd pod process on pod shutdown
  terminationGracePeriodSeconds: 30
  # If needed, more volumes can be added with this field
  extraVolumes: []
  # Values for init container
  init:
    # Docker image for the init container
    containerImage: "busybox"
    # If needed, more volume mounts to the init container can be added with this field
    extraVolumeMounts: []
  # Set priorityClassName for all DaemonSet pods
  priorityClassName: ""
  # Update strategy for the daemonset
  updateStrategy: {}

windowsDaemonset:
  # Controls whether the Daemonset is enabled for Windows
  enabled: true
  # Tolerations for the Daemonset for Windows nodes
  tolerations:
  - key: node-role.kubernetes.io/master
    effect: NoSchedule
  # Deploy the pods only on Windows nodes
  nodeSelector:
    os: windows
  # Controls whether Fluentd system messages will be enabled
  fluentdSystemdConf: disable
  # Controls the launch of a prometheus plugin that monitors Fluentd
  fluentdPrometheusConf: false
  # Use if you wish to send logs from specific k8s namespaces, space delimited.
  # Should be in the following format:
  # kubernetes.var.log.containers.**_<<NAMESPACE-TO-INCLUDE>>_** kubernetes.var.log.containers.**_<<ANOTHER-NAMESPACE>>_**.
  includeNamespace: ""
  # Enables to validate SSL certificates
  kubernetesVerifySsl: true
  # Match Fluentd's format for kube-apiserver audit logs.
  # Set to audit-json if your audit logs are in json format
  auditLogFormat: audit
  # Container runtime interface of the cluster. Used to determine which configuration to use when concatenating partial logs.
  # Valid options are: docker, containerd
  cri: containerd
  # The interval of refreshing the list of watch file for log files.
  LogFileRefreshInterval: 60s
  # Specifies which plugin to use as the backend
  logzioBufferType: file
  # Path of the buffer
  logzioBufferPath: C:\var\log\fluentd-buffers\stackdriver.buffer
  # Controls the behavior when the queue becomes full. Can be one of: throw_exception, block, drop_oldest_chunk
  logzioOverflowAction: block
  # Maximum size of a chunk allowed
  logzioChunkLimitSize: 2M
  # Maximum length of the output queue
  logzioQueueLimitLength: 6
  # Interval, in seconds, to wait before invoking the next buffer flush
  logzioFlushInterval: 5s
  # Maximum interval, in seconds, to wait between retries
  logzioRetryMaxInterval: 30
  # If true, plugin will retry flushing forever
  logzioRetryForever: true
  # Number of threads to flush the buffer
  logzioFlushThreadCount: 2
  # The log level for the Fluentd logs
  logzioLogLevel: info
  # Path to fluentd logs file, to exclude them from the logs that Fluent tails
  excludeFluentdPath: "/var/log/containers/*fluentd*.log"
  # A comma-seperated list (no spaces), of more paths to exclude from the Fluentd source that tails containers logs.
  # For example - /path/one.log,/path/two.log
  extraExclude: ""
  # Path for containers logs
  containersPath: "/var/log/containers/*.log"
  # If needed, more env vars can be added with this field
  extraEnv: []
  # Allows you to set the resources for Fluentd Daemonset.
  resources:
    requests:
      cpu: 100m
      memory: 200Mi
  # If needed, more volume mounts can be added with this field
  extraVolumeMounts: []
  # Termination period (in seconds) to wait before killing Fluentd pod process on pod shutdown
  terminationGracePeriodSeconds: 30
  # If needed, more volumes can be added with this field
  extraVolumes: []
  # Set priorityClassName for all DaemonSet pods
  priorityClassName: ""
  updateStrategy: {}

clusterRole:
  # Configurable cluster role rules that Fluentd uses to access Kubernetes resources
  rules:
  - apiGroups:
    - ""
    resources:
    - pods
    - namespaces
    verbs:
    - get
    - list
    - watch

secrets:
  # When true, the logzio secret will be created and managed by this Chart. If you're managing the logzio secret by yourself, set to false
  enabled: true
  # Secret with your logzio shipping token
  logzioShippingToken: ""
  # Secret with your logzio listener host. For example - listener.logz.io
  logzioListener: ""
  # Secret with your custom endpoint, for example:http://endpoint:8080. Overrides secrets.logzioListener
  customEndpoint: ""

# Name of the secret, can be configured in case it's placed from an external source
secretName: logzio-logs-secret

# Initial includes for fluent.conf
configMapIncludes: |
  @include "#{ENV['FLUENTD_SYSTEMD_CONF'] || 'systemd'}.conf"
  @include prometheus.conf
  @include kubernetes.conf
  @include system.conf
  @include conf.d/*.conf

configmap:
  # If needed, more Fluentd configuration can be added with this field
  extraConfig: {}
  # Configuration for fluent.conf
  fluent: |
    <match "#{ENV['INCLUDE_NAMESPACE'] || '**'}">
      @type logzio_buffered
      @id out_logzio
      endpoint_url "#{ENV['LOGZIO_LOG_LISTENER']}?token=#{ENV['LOGZIO_LOG_SHIPPING_TOKEN']}"
      output_include_time true
      output_include_tags true
      <buffer>
        # Set the buffer type to file to improve the reliability and reduce the memory consumption
        @type "#{ENV['LOGZIO_BUFFER_TYPE']}"
        path "#{ENV['LOGZIO_BUFFER_PATH']}"
        # Set queue_full action to block because we want to pause gracefully
        # in case of the off-the-limits load instead of throwing an exception
        overflow_action "#{ENV['LOGZIO_OVERFLOW_ACTION']}"
        # Set the chunk limit conservatively to avoid exceeding the GCL limit
        # of 10MiB per write request.
        chunk_limit_size "#{ENV['LOGZIO_CHUNK_LIMIT_SIZE']}"
        # Cap the combined memory usage of this buffer and the one below to
        # 2MiB/chunk * (6 + 2) chunks = 16 MiB
        queue_limit_length "#{ENV['LOGZIO_QUEUE_LIMIT_LENGTH']}"
        # Never wait more than 5 seconds before flushing logs in the non-error case.
        flush_interval "#{ENV['LOGZIO_FLUSH_INTERVAL']}"
        # Never wait longer than 30 seconds between retries.
        retry_max_interval "#{ENV['LOGZIO_RETRY_MAX_INTERVAL']}"
        # Disable the limit on the number of retries (retry forever).
        retry_forever "#{ENV['LOGZIO_RETRY_FOREVER']}"
        # Use multiple threads for processing.
        flush_thread_count "#{ENV['LOGZIO_FLUSH_THREAD_COUNT']}"
      </buffer>
    </match>

  # Configuration for kubernetes.conf
  kubernetes: |
    <label @FLUENT_LOG>
      <match fluent.**>
        @type null
      </match>
    </label>

    @include custom-sources.conf

    <source>
      @type tail
      @id in_tail_container_logs
      path "#{ENV['CONTAINERS_PATH']}"
      pos_file "#{ENV['POS_FILE_PATH']}"
      # The following line removes fluentd containers logs:
      exclude_path "#{ENV['EXCLUDE_PATH'].split(',')}"
      tag logzio.kubernetes.*
      read_from_head true
      refresh_interval "#{ENV['LOG_FILE_REFRESH_INTERVAL']}"
      <parse>
        @type multi_format
        <pattern>
          # for docker cri
          format json
          time_key time
          time_format %Y-%m-%dT%H:%M:%S.%NZ
          keep_time_key true
        </pattern>
        <pattern>
          # for containerd cri
          # format /^(?<time>.+) (?<stream>stdout|stderr) [^ ]* (?<log>.*)$/
          format /^(?<time>.+) (?<stream>stdout|stderr) (?<logtag>[FP]) (?<log>.*)$/
          time_format %Y-%m-%dT%H:%M:%S.%N%:z
          keep_time_key true
        </pattern>
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_minion
      path /var/log/salt/minion
      pos_file /var/log/fluentd-salt.pos
      tag logzio.salt
      refresh_interval "#{ENV['LOG_FILE_REFRESH_INTERVAL']}"
      <parse>
        @type regexp
        expression /^(?<time>[^ ]* [^ ,]*)[^\[]*\[[^\]]*\]\[(?<severity>[^ \]]*) *\] (?<message>.*)$/
        time_format %Y-%m-%d %H:%M:%S
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_startupscript
      path /var/log/startupscript.log
      pos_file /var/log/fluentd-startupscript.log.pos
      tag logzio.startupscript
      refresh_interval "#{ENV['LOG_FILE_REFRESH_INTERVAL']}"
      <parse>
        @type syslog
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_docker
      path /var/log/docker.log
      pos_file /var/log/fluentd-docker.log.pos
      tag logzio.docker
      refresh_interval "#{ENV['LOG_FILE_REFRESH_INTERVAL']}"
      <parse>
        @type regexp
        expression /^time="(?<time>[^)]*)" level=(?<severity>[^ ]*) msg="(?<message>[^"]*)"( err="(?<error>[^"]*)")?( statusCode=($<status_code>\d+))?/
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_etcd
      path /var/log/etcd.log
      pos_file /var/log/fluentd-etcd.log.pos
      tag logzio.etcd
      refresh_interval "#{ENV['LOG_FILE_REFRESH_INTERVAL']}"
      <parse>
        @type none
      </parse>
    </source>
    
    # This changes message key to log for etcd logs
    <filter logzio.etcd>
      @type record_modifier
      <record>
      log ${record["message"]}
      </record>
    </filter>

    <source>
      @type tail
      @id in_tail_kubelet
      multiline_flush_interval 5s
      path /var/log/kubelet.log
      pos_file /var/log/fluentd-kubelet.log.pos
      tag logzio.kubelet
      refresh_interval "#{ENV['LOG_FILE_REFRESH_INTERVAL']}"
      <parse>
        @type kubernetes
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_kube_proxy
      multiline_flush_interval 5s
      path /var/log/kube-proxy.log
      pos_file /var/log/fluentd-kube-proxy.log.pos
      tag logzio.kube-proxy
      refresh_interval "#{ENV['LOG_FILE_REFRESH_INTERVAL']}"
      <parse>
        @type kubernetes
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_kube_apiserver
      multiline_flush_interval 5s
      path /var/log/kube-apiserver.log
      pos_file /var/log/fluentd-kube-apiserver.log.pos
      tag logzio.kube-apiserver
      refresh_interval "#{ENV['LOG_FILE_REFRESH_INTERVAL']}"
      <parse>
        @type kubernetes
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_kube_controller_manager
      multiline_flush_interval 5s
      path /var/log/kube-controller-manager.log
      pos_file /var/log/fluentd-kube-controller-manager.log.pos
      tag logzio.kube-controller-manager
      refresh_interval "#{ENV['LOG_FILE_REFRESH_INTERVAL']}"
      <parse>
        @type kubernetes
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_kube_scheduler
      multiline_flush_interval 5s
      path /var/log/kube-scheduler.log
      pos_file /var/log/fluentd-kube-scheduler.log.pos
      tag logzio.kube-scheduler
      refresh_interval "#{ENV['LOG_FILE_REFRESH_INTERVAL']}"
      <parse>
        @type kubernetes
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_rescheduler
      multiline_flush_interval 5s
      path /var/log/rescheduler.log
      pos_file /var/log/fluentd-rescheduler.log.pos
      tag logzio.rescheduler
      refresh_interval "#{ENV['LOG_FILE_REFRESH_INTERVAL']}"
      <parse>
        @type kubernetes
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_glbc
      multiline_flush_interval 5s
      path /var/log/glbc.log
      pos_file /var/log/fluentd-glbc.log.pos
      tag logzio.glbc
      refresh_interval "#{ENV['LOG_FILE_REFRESH_INTERVAL']}"
      <parse>
        @type kubernetes
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_cluster_autoscaler
      multiline_flush_interval 5s
      path /var/log/cluster-autoscaler.log
      pos_file /var/log/fluentd-cluster-autoscaler.log.pos
      tag logzio.cluster-autoscaler
      refresh_interval "#{ENV['LOG_FILE_REFRESH_INTERVAL']}"
      <parse>
        @type kubernetes
      </parse>
    </source>

    @include "#{ENV['AUDIT_LOG_FORMAT'] || 'audit'}.conf"

    @include custom-filters.conf

    @include env-id.conf

    # This handles multiline exceptions automatically: https://github.com/GoogleCloudPlatform/fluent-plugin-detect-exceptions
    <match logzio.**>
      @type detect_exceptions
      remove_tag_prefix logzio
      message log
      languages all
      multiline_flush_interval 0.1
    </match>

    @include "partial-#{ENV['CRI']}.conf"

    # This adds type to the log && change key log to message
    <filter **>
      @type record_modifier
      <record>
        type "#{ENV['LOG_TYPE'] || 'k8s'}"
        message ${record["log"]}
      </record>
      remove_keys log
    </filter>

    <filter kubernetes.**>
      @type kubernetes_metadata
      @id filter_kube_metadata
      kubernetes_url "#{ENV['FLUENT_FILTER_KUBERNETES_URL'] || 'https://' + ENV.fetch('KUBERNETES_SERVICE_HOST') + ':' + ENV.fetch('KUBERNETES_SERVICE_PORT') + '/api'}"
      verify_ssl "#{ENV['KUBERNETES_VERIFY_SSL'] || true}"
      annotation_match "log\_type|logz\.io\/application\_type"
    </filter>
    <filter **>
      @type record_modifier
      <record>
        # use regex to find log level keywords and extract them
        dummy_level ${record.dig("message").match(/(?i)(?<ss>(\b(info|debug|warning|warn|error|panic|failure|failed|exception|trace)\b))/).to_s}
        # sort log level keywords and use info as default
        log_level ${if record.dig("dummy_level") =~ /^(?i)(error|failure|failed|exception|panic)$/ then "ERROR"; elsif record.dig("dummy_level")=~ /^(?i)(warn|warning)$/ then "WARNING" ; elsif record.dig("dummy_level") != "" then record.dig("dummy_level").upcase; else "INFO" end;}
        # insert log_type from annotation (if exists)
        type ${record.dig("kubernetes","annotations","log_type") || record.dig("kubernetes","annotations","logz.io/application_type") || record.dig("type")}
        _dummy_ ${if record.dig("kubernetes","annotations","log_type") then record["kubernetes"]["annotations"].delete("log_type"); end;}
      </record>
      remove_keys _dummy_,dummy_level
    </filter>
    <filter **>
      @type             dedot
      de_dot            true
      de_dot_separator  _
      de_dot_nested     true
    </filter>

    @include log-level-filter.conf

    @include custom-filters-after.conf

  # Configuration for system.conf
  system: |
    <system>
      log_level "#{ENV['LOGZIO_LOG_LEVEL']}"
    </system>

  # Configuration for systemd.conf
  systemd: |
    # Logs from systemd-journal for interesting services.
    <source>
      @type systemd
      @id in_systemd_kubelet
      filters [{ "_SYSTEMD_UNIT": "kubelet.service" }]
      <storage>
        @type local
        persistent true
        path /var/log/fluentd-journald-kubelet-cursor.json
      </storage>
      read_from_head true
      tag kubelet
    </source>

    # Logs from docker-systemd
    <source>
      @type systemd
      @id in_systemd_docker
      filters [{ "_SYSTEMD_UNIT": "docker.service" }]
      <storage>
        @type local
        persistent true
        path /var/log/fluentd-journald-docker-cursor.json
      </storage>
      read_from_head true
      tag docker.systemd
    </source>

    # Logs from systemd-journal for interesting services.
    <source>
      @type systemd
      @id in_systemd_bootkube
      filters [{ "_SYSTEMD_UNIT": "bootkube.service" }]
      <storage>
        @type local
        persistent true
        path /var/log/fluentd-journald-bootkube-cursor.json
      </storage>
      read_from_head true
      tag bootkube
    </source>

  # Configuration for audit.conf
  audit: |
    # Example:
    # 2017-02-09T00:15:57.992775796Z AUDIT: id="90c73c7c-97d6-4b65-9461-f94606ff825f" ip="104.132.1.72" method="GET" user="kubecfg" as="<self>" asgroups="<lookup>" namespace="default" uri="/api/v1/namespaces/default/pods"
    # 2017-02-09T00:15:57.993528822Z AUDIT: id="90c73c7c-97d6-4b65-9461-f94606ff825f" response="200"
    <source>
      @type tail
      @id in_tail_kube_apiserver_audit
      multiline_flush_interval 5s
      path /var/log/kubernetes/kube-apiserver-audit.log
      pos_file /var/log/kube-apiserver-audit.log.pos
      tag logzio.kube-apiserver-audit
      refresh_interval "#{ENV['LOG_FILE_REFRESH_INTERVAL']}"
      <parse>
        @type multiline
        format_firstline /^\S+\s+AUDIT:/
        # Fields must be explicitly captured by name to be parsed into the record.
        # Fields may not always be present, and order may change, so this just looks
        # for a list of key="\"quoted\" value" pairs separated by spaces.
        # Unknown fields are ignored.
        # Note: We can't separate query/response lines as format1/format2 because
        #       they don't always come one after the other for a given query.
        format1 /^(?<time>\S+) AUDIT:(?: (?:id="(?<id>(?:[^"\\]|\\.)*)"|ip="(?<ip>(?:[^"\\]|\\.)*)"|method="(?<method>(?:[^"\\]|\\.)*)"|user="(?<user>(?:[^"\\]|\\.)*)"|groups="(?<groups>(?:[^"\\]|\\.)*)"|as="(?<as>(?:[^"\\]|\\.)*)"|asgroups="(?<asgroups>(?:[^"\\]|\\.)*)"|namespace="(?<namespace>(?:[^"\\]|\\.)*)"|uri="(?<uri>(?:[^"\\]|\\.)*)"|response="(?<response>(?:[^"\\]|\\.)*)"|\w+="(?:[^"\\]|\\.)*"))*/
        time_format %Y-%m-%dT%T.%L%Z
      </parse>
    </source>

  # Configuration for audit-json.conf. This is the configuration that's being used when daemonset.auditLogFormat is set to audit-json
  auditJson: |
    <source>
      @type tail
      @id in_tail_kube_apiserver_audit
      multiline_flush_interval 5s
      path /var/log/kubernetes/kube-apiserver-audit.log
      pos_file /var/log/kube-apiserver-audit.log.pos
      tag logzio.kube-apiserver-audit
      refresh_interval "#{ENV['LOG_FILE_REFRESH_INTERVAL']}"
      <parse>
        @type json
        keep_time_key true
        time_key timestamp
        time_format %Y-%m-%dT%T.%L%Z
      </parse>
    </source>

  # Configuration for partial-docker.conf. Used to concatenate partial logs that split due to large size, for docker cri
  partialDocker: |
    # Concat docker cri partial log
    # https://github.com/fluent-plugins-nursery/fluent-plugin-concat
    # https://github.com/moby/moby/issues/34620#issuecomment-619369707
    <filter **>
      @type concat
      key log
      use_first_timestamp true
      multiline_end_regexp /\n$/
      separator ""
    </filter>

  # Configuration for partial-containerd.conf. Used to concatenate partial logs that split due to large size, for containerd cri
  partialContainerd: |
    # Concat containerd cri partial log
    # https://github.com/fluent/fluentd-kubernetes-daemonset/issues/412#issuecomment-636536767
    <filter **>
      @type concat
      key log
      use_first_timestamp true
      partial_key logtag
      partial_value P
      separator ""
    </filter>

  # Config snippet for adding env_id field to logs
  envId: |
    <filter **>
        @type record_modifier
        <record>
          env_id "#{ENV['ENV_ID']}"
        </record>
      </filter>
  
  # Config for prometheus.conf
  prometheus: |
    <source>
      @type prometheus
      @id in_prometheus
      bind "0.0.0.0"
      port 24231
      metrics_path "/metrics"
    </source>
    <filter **>
      @type prometheus
      <metric>
        name fluentd_input_status_num_records_total
        type counter
        desc The total number of incoming records
        <labels>
          tag ${tag}
          hostname ${hostname}
        </labels>
      </metric>
    </filter>
    <source>
      @type prometheus_monitor
      @id in_prometheus_monitor
    </source>
    <source>
      @type prometheus_output_monitor
      @id in_prometheus_output_monitor
    </source>
  
  # Add sources to the Fluentd configuration
  customSources: ""
  # Add filters to the Fluentd configuration, before default filters
  customFilters: ""
  # Add filters to the Fluentd configuration, after default filters
  customFilterAfter: ""

# Add log level filter. Regex of the log level(s) you want to ship.
# For example, if you want to ship warning and error logs, use WARNING|ERROR. Possible levels are: DEBUG, INFO, WARNING, ERROR, TRACE.
logLevelFilter: ""

