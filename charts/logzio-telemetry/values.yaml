# Default values for logzio-telemetry.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

collector:
  mode: daemonset # possible values: standalone, daemonset
traces:
  enabled: false
metrics:
  enabled: false
spm:
  enabled: false
applicationMetrics:
  enabled: false
nameOverride: "otel-collector"
fullnameOverride: ""

secrets:
  name: logzio-secret
  enabled: true
  MetricsToken: ""
  TracesToken: ""
  SpmToken: ""
  ListenerHost: ""
  env_id: "my_environment"
  LogzioRegion: "us"
  CustomTracingEndpoint: ""
  p8s_logzio_name: ""
  windowsNodeUsername: ""
  windowsNodePassword: ""
  SamplingProbability: 10
  SamplingLatency: 500


managedServiceAccount: true

clusterRole:
  # Specifies whether a clusterRole should be created
  create: false
  # Annotations to add to the clusterRole
  annotations: {}
  # The name of the clusterRole to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""
  # A set of rules as documented here : https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  rules: []
  # - apiGroups:
  #   - ''
  #   resources:
  #   - 'pods'
  #   - 'nodes'
  #   verbs:
  #   - 'get'
  #   - 'list'
  #   - 'watch'
  clusterRoleBinding:
    # Annotations to add to the clusterRoleBinding
    annotations: {}
    # The name of the clusterRoleBinding to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""

clusterRoleRules:
- apiGroups:
  - ""
  resources:
  - events
  - namespaces
  - namespaces/status
  - nodes
  - nodes/spec
  - pods
  - pods/metrics
  - nodes/metrics
  - pods/status
  - replicationcontrollers
  - replicationcontrollers/status
  - resourcequotas
  - services
  - endpoints
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - apps
  resources:
  - daemonsets
  - deployments
  - replicasets
  - statefulsets
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - extensions
  resources:
  - daemonsets
  - deployments
  - replicasets
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - batch
  resources:
  - jobs
  - cronjobs
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - autoscaling
  resources:
  - horizontalpodautoscalers
  verbs:
  - get
  - list
  - watch


tags:
  kubeStateMetrics:
    ## If false, kube-state-metrics sub-chart will not be installed
    ##
    enabled: true

  pushGateway:
    ## If false, pushGateway sub-chart will not be installed
    ##
    enabled: true
  nodeExporter:
    ## If false, node-exporter will not be installed
    ##
    enabled: true
    
prometheus-pushgateway:
  serviceAnnotations:
    prometheus.io/scrape: "true"
  nodeSelector:
    kubernetes.io/os: linux

emptyConfig: {}

baseCollectorConfig:
  exporters:
    logging:
      loglevel: info
  extensions:
    health_check: {}
  processors:
    k8sattributes:
      extract:
        metadata:
        - k8s.pod.name
        - k8s.deployment.name
        - k8s.namespace.name
        - k8s.node.name
        - k8s.statefulset.name
        - k8s.replicaset.name
        - k8s.daemonset.name
        - k8s.cronjob.name
        - k8s.job.name
    resource/k8s:
      attributes:
      # Rename
      - key: pod
        action: insert
        from_attribute: k8s.pod.name
      - key: kubernetes_node
        action: insert
        from_attribute: k8s.node.name
      - key: kubernetes_namespace
        action: insert
        from_attribute: k8s.namespace.name
      - key: kubernetes_deployment
        action: insert
        from_attribute: k8s.deployment.name
      - key: kubernetes_pod_ip
        action: insert
        from_attribute: k8s.pod.ip
      - key: kubernetes_statefulset
        action: insert
        from_attribute: k8s.statefulset.name
      - key: kubernetes_replicaset
        action: insert
        from_attribute: k8s.replicaset.name
      - key: kubernetes_cronjob
        action: insert
        from_attribute: k8s.cronjob.name
      - key: kubernetes_daemonset
        action: insert
        from_attribute: k8s.daemonset.name
      - key: kubernetes_job
        action: insert
        from_attribute: k8s.job.name
      # Delete old
      - key: k8s.deployment.name
        action: delete
      - key: k8s.pod.name
        action: delete
      - key: k8s.namespace.name
        action: delete
      - key: k8s.node.name
        action: delete
      - key: k8s.pod.ip
        action: delete
      - key: k8s.statefulset.name
        action: delete
      - key: k8s.replicaset.name
        action: delete
      - key: k8s.daemonset.name
        action: delete
      - key: k8s.job.name
        action: delete
      - key: k8s.cronjob.name
        action: delete
    batch: {}
    attributes/env_id:
      actions:
        - key: env_id
          value: ${ENV_ID}
          action: insert
        - key: logzio_agent_version
          value: ${LOGZIO_AGENT_VERSION}
          action: insert
  service:
    extensions:
      - health_check
    telemetry:
      logs:
        level: "info"

tracesConfig:
  exporters:
    logzio:
      endpoint: ${CUSTOM_TRACING_ENDPOINT}
      region: ${LOGZIO_LISTENER_REGION}
      account_token: ${TRACES_TOKEN}
    logging:
      loglevel: info
  extensions:
    pprof:
      endpoint: :1777
    zpages:
      endpoint: :55679
  receivers:
    jaeger:
      protocols:
        thrift_compact:
          endpoint: "0.0.0.0:6831"
        thrift_binary:
          endpoint: "0.0.0.0:6832"
        grpc:
          endpoint: "0.0.0.0:14250"
        thrift_http:
          endpoint: "0.0.0.0:14268"
    otlp:
      protocols:
        grpc:
          endpoint: "0.0.0.0:4317"
        http:
          endpoint: "0.0.0.0:4318"
    zipkin:
      endpoint: "0.0.0.0:9411"
  processors:
    resourcedetection/all:
      detectors: [ec2, azure, gcp]
    tail_sampling:
      policies:
        [
            {
              name: error-in-policy,
              type: status_code,
              status_code: {status_codes: [ERROR]}
            },
            {
              name: slow-traces-policy,
              type: latency,
              latency: {threshold_ms: "${SAMPLING_LATENCY}" }            
            },
            {
              name: probability-policy,
              type: probabilistic,
              probabilistic: {sampling_percentage: "${SAMPLING_PROBABILITY}" }
            }       
        ]
  service:
    extensions:
      - health_check
      - pprof
      - zpages
    pipelines:
      traces:
        receivers: [jaeger, zipkin, otlp]
        processors: [resourcedetection/all,attributes/env_id, k8sattributes, resource/k8s, tail_sampling, batch]
        exporters: [logzio]
    telemetry:
      logs:
        level: "info"  

spmForwarderConfig:
  exporters:
    otlp:
      endpoint: "${SPM_SERVICE_ENDPOINT}"
      tls:
        insecure: true
  service:
    pipelines:
      traces/spm:
        receivers: [jaeger, zipkin, otlp]
        processors: [resourcedetection/all, attributes/env_id, k8sattributes]
        exporters: [logging, otlp]

metricsConfig:
  extensions:
    health_check: {}
  processors:
    filter/kubernetes360:
      metrics:
        datapoint:
          - 'IsMatch(metric.name, "(${K8S_360_METRICS})") == true and attributes["logzio_app"] != "kubernetes360"'
  exporters:
    prometheusremotewrite/applications:
      timeout: 30s
      endpoint: ${LISTENER_URL}
      external_labels:
        p8s_logzio_name: ${P8S_LOGZIO_NAME}
      headers:
        Authorization: "Bearer ${METRICS_TOKEN}"
    prometheusremotewrite/infrastructure:
      timeout: 30s
      endpoint: ${LISTENER_URL}
      external_labels:
        p8s_logzio_name: ${P8S_LOGZIO_NAME}
      headers:
        Authorization: "Bearer ${METRICS_TOKEN}"
  receivers:
    prometheus/applications:
      config:
        global:
          scrape_interval: 60s
          scrape_timeout: 60s
        scrape_configs:
        - job_name: applications
          honor_timestamps: true
          honor_labels: true
          metrics_path: /metrics
          scheme: http
          kubernetes_sd_configs:
          - role: pod
          relabel_configs:
            - action: keep
              regex: true|"true"
              source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            - action: replace
              regex: (https?)
              source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]
              target_label: __scheme__
            - action: replace
              regex: (.+)
              source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
              target_label: __metrics_path__
            - action: replace
              regex: (.+?)(?::\d+)?;(\d+)
              replacement: $$1:$$2
              source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
              target_label: __address__
            - action: labelmap
              regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)
              replacement: __param_$$1
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - action: replace
              source_labels: [__meta_kubernetes_namespace]
              target_label: namespace
            - action: replace
              source_labels: [__meta_kubernetes_pod_name]
              target_label: pod
            - source_labels: [__meta_kubernetes_pod_node_name]
              action: replace
              target_label: kubernetes_node          
          metric_relabel_configs: []
    prometheus/cadvisor:
      config:
        global:
          scrape_interval: 30s
          scrape_timeout: 30s
        scrape_configs:
        - job_name: 'kubernetes-cadvisor'
          scheme: https
          metrics_path: /metrics/cadvisor
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            insecure_skip_verify: true
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
          kubernetes_sd_configs:
          - role: node
          relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)  
          metric_relabel_configs: []
    prometheus/collector:
      config:
        global:
          scrape_interval: 30s
          scrape_timeout: 30s
        scrape_configs:
        # Job to collect opentelemetry collector metrics
        - job_name: 'collector-metrics'
          scrape_interval: 15s
          static_configs:
          - targets: [ "0.0.0.0:8888" ]
    prometheus/infrastructure:
      config:
        global:
          scrape_interval: 30s
          scrape_timeout: 30s
        scrape_configs:
        - job_name: windows-metrics
          honor_timestamps: true
          honor_labels: true
          metrics_path: /metrics
          scheme: http
          kubernetes_sd_configs:
          - role: pod
          relabel_configs:
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_windows_io_scrape]
              action: keep
              regex: true|"true"
          metric_relabel_configs: []
        - job_name: kubernetes-service-endpoints
          honor_timestamps: true
          honor_labels: true
          metrics_path: /metrics
          scheme: http
          kubernetes_sd_configs:
          - role: endpoints
          relabel_configs:
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
              action: keep
              regex: true|"true"
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
              action: replace
              target_label: __scheme__
              regex: (https?)
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
              action: replace
              target_label: __address__
              regex: ([^:]+)(?::\d+)?;(\d+)
              replacement: $$1:$$2
            - action: labelmap
              regex: __meta_kubernetes_service_label_(.+)
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: namespace
            - source_labels: [__meta_kubernetes_service_name]
              action: replace
              target_label: service
            - source_labels: [__meta_kubernetes_pod_name]
              action: replace
              target_label: pod
            - source_labels: [__meta_kubernetes_pod_node_name]
              action: replace
              target_label: kubernetes_node
            - source_labels: [__meta_kubernetes_service_annotation_logz_io_app]
              action: replace
              target_label: logzio_app                 
          metric_relabel_configs: []
  service:
    extensions:
      - health_check
    pipelines:
      metrics/infrastructure:
        exporters:
          - prometheusremotewrite/infrastructure
        processors:
          - attributes/env_id
          - filter/kubernetes360
        receivers:
          - prometheus/infrastructure
          - prometheus/cadvisor
          - prometheus/collector
          
# Shared params for daemonsetCollector and standaloneCollector deployment pods.
# Can be overridden here or for any component independently using the same keys.

image:
  # If you want to use the contrib image `otel/opentelemetry-collector-contrib`, you also need to change `command.name` value to `otelcontribcol`.
  repository: otel/opentelemetry-collector-contrib
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "0.80.0"
nginxWindowsImage:
  # Reverse proxy image to enable metrics scraping from windows nodes
  repository: logzio/logzio-windows-node-reverse-proxy
  pullPolicy: IfNotPresent
  tag: "0.0.1"
windowsExporterInstallerImage:
  # Job image to install windows exporter on windows nodes
  repository: logzio/logzio-windows-exporter-installer
  pullPolicy: IfNotPresent
  tag: "0.0.1"
spmImage:
  repository: otel/opentelemetry-collector-contrib
  pullPolicy: IfNotPresent
  tag: "0.73.0"
imagePullSecrets: []

# OpenTelemetry Collector executable
command:
  name: otelcol-contrib
  extraArgs: 

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podSecurityContext: {}
securityContext: {}

nodeSelector: {}
linuxNodeSelector: {
  "kubernetes.io/os": linux
}

tolerations: []

affinity: {}

extraEnvs: []
extraHostPathMounts: []
secretMounts: []

# Configuration for ports, shared between daemonsetCollector, standaloneCollector and service.
# Can be overridden here or for daemonsetCollector and standaloneCollector independently.
ports:
  otlp:
    enabled: true
    containerPort: 4317
    servicePort: 4317
    hostPort: 4317
    protocol: TCP
  otlp-http:
    enabled: true
    containerPort: 4318
    servicePort: 4318
    hostPort: 4318
    protocol: TCP
  jaeger-compact:
    enabled: true
    containerPort: 6831
    servicePort: 6831
    hostPort: 6831
    protocol: UDP
  jaeger-thrift:
    enabled: true
    containerPort: 14268
    servicePort: 14268
    hostPort: 14268
    protocol: TCP
  jaeger-grpc:
    enabled: true
    containerPort: 14250
    servicePort: 14250
    hostPort: 14250
    protocol: TCP
  zipkin:
    enabled: true
    containerPort: 9411
    servicePort: 9411
    hostPort: 9411
    protocol: TCP

spanMetricsAgregator:
  config:
    processors:
      spanmetrics:
        metrics_exporter: prometheus/spm
        latency_histogram_buckets: [2ms, 8ms, 50ms, 100ms, 200ms, 500ms, 1s, 5s, 10s]
        dimensions_cache_size: 100000
        aggregation_temporality: AGGREGATION_TEMPORALITY_CUMULATIVE
        dimensions:
          - name: http.method
          - name: http.status_code
          - name: k8s.pod.name
          - name: k8s.deployment.name
          - name: k8s.namespace.name
          - name: k8s.node.name
          - name: k8s.statefulset.name
          - name: k8s.replicaset.name
          - name: k8s.daemonset.name
          - name: k8s.cronjob.name
          - name: k8s.job.name
          - name: cloud.provider
          - name: cloud.region
          - name: env_id
            default: ${ENV_ID}
    receivers:
      # Dummy recivier 
      otlp/spm:
        protocols:
          grpc:
            endpoint: "localhost:12345"
      prometheus/spm-logzio:
        config:
          scrape_configs:
          - job_name: 'spm'
            scrape_interval: 30s
            static_configs:
            - targets: [ "0.0.0.0:8889" ]
      jaeger:
        protocols:
          thrift_compact:
            endpoint: "0.0.0.0:6831"
          thrift_binary:
            endpoint: "0.0.0.0:6832"
          grpc:
            endpoint: "0.0.0.0:14250"
          thrift_http:
            endpoint: "0.0.0.0:14268"
      otlp:
        protocols:
          grpc:
            endpoint: "0.0.0.0:4317"
          http:
            endpoint: "0.0.0.0:4318"
      zipkin:
        endpoint: "0.0.0.0:9411"
    exporters:
      logging:
        loglevel: info
      prometheus/spm:
        endpoint: "0.0.0.0:8889"
      prometheusremotewrite/spm-logzio:
        timeout: 30s
        endpoint: ${LISTENER_URL}
        headers:
          Authorization: "Bearer ${SPM_TOKEN}"
    service:
      pipelines:
        traces:
          receivers: [jaeger, zipkin, otlp]
          processors: [spanmetrics]
          exporters: [logging]
        metrics/spm:
          # Dummy recivier 
          receivers: [otlp/spm]
          exporters: [prometheus/spm]
        metrics/spm-logzio:
          receivers: [prometheus/spm-logzio]
          exporters: [prometheusremotewrite/spm-logzio]
  # service values        
  service:
    type: ClusterIP
    annotations: {}
  ports:
    otlp:
      enabled: true
      containerPort: 4317
      servicePort: 4317
      hostPort: 4317
      protocol: TCP
    otlp-http:
      enabled: true
      containerPort: 4318
      servicePort: 4318
      hostPort: 4318
      protocol: TCP
    jaeger-compact:
      enabled: true
      containerPort: 6831
      servicePort: 6831
      hostPort: 6831
      protocol: UDP
    jaeger-thrift:
      enabled: true
      containerPort: 14268
      servicePort: 14268
      hostPort: 14268
      protocol: TCP
    jaeger-grpc:
      enabled: true
      containerPort: 14250
      servicePort: 14250
      hostPort: 14250
      protocol: TCP
    zipkin:
      enabled: true
      containerPort: 9411
      servicePort: 9411
      hostPort: 9411
      protocol: TCP
  resources:
    limits:
      cpu: 256m
      memory: 512Mi


# Configuration for standalone OpenTelemetry Collector deployment, enabled by default
standaloneCollector:
  enabled: false

  containerLogs:
    enabled: false

  resources:
    limits:
      memory: 512Mi
    requests:
      cpu: 200m

  podAnnotations: {}
  # Configuration override that will be merged into the colelctor default config
  configOverride: {}

daemonsetCollector:
  enabled: false

  containerLogs:
    enabled: false
  
  # prevent collector daemonset deployment on fargate nodes
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: eks.amazonaws.com/compute-type
                operator: DoesNotExist

  resources:
    limits:
      memory: 250Mi
    requests:
      cpu: 150m

  podAnnotations: {}
  # Configuration override that will be merged into the daemonset default config
  configOverride: {}

service:
  type: ClusterIP
  annotations: {}

# autoscaling is used only if standaloneCollector enabled
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

windowsExporterInstallerJob:
  interval: "*/10 * * * *"
  concurrencyPolicy: Forbid            # Future cronjob will run only after current job is finished
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  ttlSecondsAfterFinished: 3600        # First job only (Not CronJob)   

podLabels:  {}
annotations:  {}

kube-state-metrics:
  service:
    annotations:
      logz.io/app: "kubernetes360"
  nodeSelector:
    kubernetes.io/os: linux
  podSecurityPolicy:
    enabled: false
  extraArgs: ["--metric-labels-allowlist=nodes=[kubernetes.azure.com/scalesetpriority,eks.amazonaws.com/capacityType,cloud.google.com/gke-preemptible]"]

prometheus-node-exporter:
  service:
    port: 9101
    targetPort: 9101
    annotations:
      prometheus.io/scrape: "true"
      logz.io/app: "kubernetes360"
  nodeSelector:
    kubernetes.io/os: linux
  # Prevent node exporter deamonset deploymment on fargate nodes
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: eks.amazonaws.com/compute-type
                operator: DoesNotExist
  rbac:
    pspEnabled: false

enableMetricsFilter:
  gke: false
  eks: false
  aks: false
  dropKubeSystem: false

disableKubeDnsScraping: false

daemonsetConfig:
  extensions:
    health_check: {}
  processors:
    resourcedetection/all:
      detectors: [ec2, azure, gcp]
    filter/kubernetes360:
      metrics:
        datapoint:
          - 'IsMatch(metric.name, "(${K8S_360_METRICS})") == true and attributes["logzio_app"] != "kubernetes360"'
          - 'IsMatch(metric.name, "(${K8S_360_METRICS})") == true and attributes["kubernetes_node"] == nil'
  exporters:
    prometheusremotewrite/applications:
      timeout: 30s
      endpoint: ${LISTENER_URL}
      external_labels:
        p8s_logzio_name: ${P8S_LOGZIO_NAME}
      headers:
        Authorization: "Bearer ${METRICS_TOKEN}"
    prometheusremotewrite/infrastructure:
      timeout: 30s
      endpoint: ${LISTENER_URL}
      external_labels:
        p8s_logzio_name: ${P8S_LOGZIO_NAME}
      headers:
        Authorization: "Bearer ${METRICS_TOKEN}"
  receivers:
    prometheus/applications:
      config:
        global:
          scrape_interval: 30s
          scrape_timeout: 30s
        scrape_configs:
        - job_name: applications
          honor_timestamps: true
          honor_labels: true
          metrics_path: /metrics
          scheme: http
          kubernetes_sd_configs:
          - role: pod
            selectors:
            # only scrape data from pods running on the same node as the collector
            - role: pod
              field: "spec.nodeName=$KUBE_NODE_NAME"
          relabel_configs:
            - action: keep
              regex: true|"true"
              source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            - action: replace
              regex: (https?)
              source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]
              target_label: __scheme__
            - action: replace
              regex: (.+)
              source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
              target_label: __metrics_path__
            - action: replace
              regex: (.+?)(?::\d+)?;(\d+)
              replacement: $$1:$$2
              source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
              target_label: __address__
            - action: labelmap
              regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)
              replacement: __param_$$1
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - action: replace
              source_labels: [__meta_kubernetes_namespace]
              target_label: namespace
            - action: replace
              source_labels: [__meta_kubernetes_pod_name]
              target_label: pod
            - source_labels: [__meta_kubernetes_pod_node_name]
              action: replace
              target_label: kubernetes_node          
          metric_relabel_configs: []
    prometheus/cadvisor:
      config:
        global:
          scrape_interval: 30s
          scrape_timeout: 30s
        scrape_configs:
        - job_name: 'kubernetes-cadvisor'
          scheme: https
          metrics_path: /metrics/cadvisor
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            insecure_skip_verify: true
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
          kubernetes_sd_configs:
          - role: node
            selectors:
            # only scrape data from node with the same name as the node the collector run on
            - role: node
              field: "metadata.name=$KUBE_NODE_NAME"
          relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+) 
          metric_relabel_configs: []
    prometheus/collector:
      config:
        global:
          scrape_interval: 30s
          scrape_timeout: 30s
        scrape_configs:
        # Job to collect opentelemetry collector metrics
        - job_name: 'collector-metrics'
          scrape_interval: 15s
          static_configs:
          - targets: [ "0.0.0.0:8888" ] 
          metric_relabel_configs: []
    prometheus/infrastructure:
      config:
        global:
          scrape_interval: 30s
          scrape_timeout: 30s
        scrape_configs:
        - job_name: windows-metrics
          honor_timestamps: true
          honor_labels: true
          metrics_path: /metrics
          scheme: http
          kubernetes_sd_configs:
          - role: pod
            selectors:
            # only scrape data from pods running on the same node as collector
            - role: pod
              field: "spec.nodeName=$KUBE_NODE_NAME"
          relabel_configs:
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_windows_io_scrape]
              action: keep
              regex: true|"true"
          metric_relabel_configs: []
        # Job to collect metrics from applications running on pods
        - job_name: kubernetes-service-endpoints
          honor_timestamps: true
          honor_labels: true
          metrics_path: /metrics
          scheme: http
          kubernetes_sd_configs:
          - role: endpoints
            selectors:
            # only scrape data from pods running on the same node as collector
            - role: pod
              field: "spec.nodeName=$KUBE_NODE_NAME"
          relabel_configs:
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
              action: keep
              regex: true|"true"
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
              action: replace
              target_label: __scheme__
              regex: (https?)
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
              action: replace
              target_label: __address__
              regex: ([^:]+)(?::\d+)?;(\d+)
              replacement: $$1:$$2
            - action: labelmap
              regex: __meta_kubernetes_service_label_(.+)
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: namespace
            - source_labels: [__meta_kubernetes_service_name]
              action: replace
              target_label: service
            - source_labels: [__meta_kubernetes_pod_name]
              action: replace
              target_label: pod
            - source_labels: [__meta_kubernetes_pod_node_name]
              action: replace
              target_label: kubernetes_node
            - source_labels: [__meta_kubernetes_service_annotation_logz_io_app]
              action: replace
              target_label: logzio_app                 
          metric_relabel_configs: []
  service:
    extensions:
      - health_check
    pipelines:
      metrics/infrastructure:
        exporters:
          - prometheusremotewrite/infrastructure
        processors:
          - attributes/env_id
          - filter/kubernetes360
        receivers:
          - prometheus/infrastructure
          - prometheus/cadvisor
          - prometheus/collector

         
opencost:
  enabled: false
  config:
    processors:
    # opencost collects duplicates metrics from kube-state and cadvisor.
      filter/opencost-exporter: 
        metrics:
          datapoint:     
            - 'IsMatch(metric.name, "(${OPENCOST_DUPLICATES})") == true and attributes["app"] == "opencost"'
    service:
      pipelines:
        metrics/infrastructure:
          processors:
            - filter/opencost-exporter

prometheusFilters:
  # Metrics names to be filtered
  # All values should be listed with | seperator, as regex. i.e: metric_1|metric_2|metric_3
  metrics:
    # for infrastructure pipeline: metrics/infrastructure & metrics/cadvisor receivers
    # (kubernetes-service-endpoints & cadvisor jobs)
    infrastructure:
      keep:
        # need to also enable the flag: enableMetricsFilter.aks=true
        aks: kube_daemonset_labels|kube_daemonset_status_number_ready|kube_daemonset_status_number_available|kube_daemonset_status_number_unavailable|kube_daemonset_status_current_number_scheduled|kube_daemonset_status_number_misscheduled|kube_daemonset_status_desired_number_scheduled|kube_job_labels|kube_job_complete|kube_job_status_failed|kube_job_status_succeeded|kube_job_complete|kube_job_status_failed|kube_job_status_completion_time|kube_replicaset_labels|kube_replicaset_spec_replicas|kube_replicaset_status_replicas|kube_replicaset_status_ready_replicas|kube_statefulset_replicas|kube_statefulset_status_replicas|kube_statefulset_status_replicas_updated|kube_statefulset_status_replicas_available|kube_pod_container_status_terminated_reason|kube_node_labels|kube_pod_container_status_waiting_reason|node_memory_Buffers_bytes|node_memory_Cached_bytes|kube_deployment_labels|container_cpu_usage_seconds_total|container_memory_working_set_bytes|container_network_receive_bytes_total|container_network_transmit_bytes_total|i:|kube_deployment_status_replicas|kube_deployment_status_replicas_available|kube_deployment_status_replicas_unavailable|kube_deployment_status_replicas_updated|kube_node_info|kube_node_spec_unschedulable|kube_node_status_allocatable|kube_node_status_capacity|kube_node_status_condition|kube_pod_container_info|kube_pod_container_resource_requests|kube_pod_container_resource_requests_cpu_cores|kube_pod_container_resource_requests_memory_bytes|kube_pod_container_status_ready|kube_pod_container_status_restarts_total|kube_pod_container_status_running|kube_pod_container_status_terminated|kube_pod_container_status_waiting|kube_pod_info|kube_pod_status_phase|machine_cpu_cores|namespace|node_boot_time_seconds|node_cpu_seconds_total|node_disk_io_time_seconds_total|node_filesystem_avail_bytes|node_filesystem_free_bytes|node_filesystem_size_bytes|node_memory_MemFree_bytes|node_memory_MemTotal_bytes|node_network_receive_bytes_total|node_network_transmit_bytes_total|node_time_seconds|p8s_logzio_name|windows_container_cpu_usage_seconds_total|windows_container_memory_usage_commit_bytes|windows_container_network_receive_bytes_total|windows_container_network_transmit_bytes_total|windows_cpu_time_total|windows_cs_hostname|windows_cs_physical_memory_bytes|windows_logical_disk_free_bytes|windows_logical_disk_read_seconds_total|windows_logical_disk_size_bytes|windows_logical_disk_write_seconds_total|windows_net_bytes_received_total|windows_net_bytes_sent_total|windows_os_physical_memory_free_bytes|windows_system_system_up_time|kube_pod_status_ready|kube_pod_container_status_restarts_total|kube_pod_container_resource_limits|container_memory_usage_bytes|container_network_transmit_packets_total|container_network_receive_packets_total|container_network_transmit_packets_dropped_total|container_network_receive_packets_dropped_total|kube_pod_created|kube_pod_owner|kube_pod_status_reason|node_cpu_seconds_total|node_memory_MemAvailable_bytes|kube_node_role|kube_node_created|node_load1|node_load5|node_load15|node_disk_reads_completed_total|node_disk_writes_completed_total|node_disk_read_bytes_total|node_disk_written_bytes_total|node_disk_read_time_seconds_total|node_disk_write_time_seconds_total|node_network_transmit_packets_total|node_network_receive_packets_total|node_network_transmit_drop_total|node_network_receive_drop_total|kube_replicaset_owner|kube_deployment_created|kube_deployment_status_condition|kube_deployment_spec_replicas|kube_namespace_status_phase|kubelet_volume_stats_inodes|kubelet_volume_stats_inodes_used|kubelet_volume_stats_used_bytes|kube_persistentvolumeclaim_info|kube_persistentvolumeclaim_resource_requests_storage_bytes|kube_job_owner
        # need to also enable the flag: enableMetricsFilter.eks=true
        eks: kube_daemonset_labels|kube_daemonset_status_number_ready|kube_daemonset_status_number_available|kube_daemonset_status_number_unavailable|kube_daemonset_status_current_number_scheduled|kube_daemonset_status_number_misscheduled|kube_daemonset_status_desired_number_scheduled|kube_job_labels|kube_job_complete|kube_job_status_failed|kube_job_status_succeeded|kube_job_complete|kube_job_status_failed|kube_job_status_completion_time|kube_replicaset_labels|kube_replicaset_spec_replicas|kube_replicaset_status_replicas|kube_replicaset_status_ready_replicas|kube_statefulset_replicas|kube_statefulset_status_replicas|kube_statefulset_status_replicas_updated|kube_statefulset_status_replicas_available|kube_pod_container_status_terminated_reason|kube_node_labels|kube_pod_container_status_waiting_reason|node_memory_Buffers_bytes|node_memory_Cached_bytes|kube_deployment_labels|container_cpu_usage_seconds_total|container_memory_working_set_bytes|container_network_receive_bytes_total|container_network_transmit_bytes_total|kube_deployment_status_replicas|kube_deployment_status_replicas_available|kube_deployment_status_replicas_unavailable|kube_deployment_status_replicas_updated|kube_node_info|kube_node_spec_unschedulable|kube_node_status_allocatable|kube_node_status_capacity|kube_node_status_condition|kube_pod_container_resource_requests|kube_pod_container_status_ready|kube_pod_container_status_restarts_total|kube_pod_container_status_running|kube_pod_container_status_terminated|kube_pod_container_status_waiting|kube_pod_info|kube_pod_status_phase|machine_cpu_cores|namespace|node_boot_time_seconds|node_cpu_seconds_total|node_disk_io_time_seconds_total|node_filesystem_avail_bytes|node_filesystem_free_bytes|node_filesystem_size_bytes|node_memory_MemFree_bytes|node_memory_MemTotal_bytes|node_network_receive_bytes_total|node_network_transmit_bytes_total|node_time_seconds|p8s_logzio_name|kube_pod_status_ready|kube_pod_container_status_restarts_total|kube_pod_container_resource_limits|container_memory_usage_bytes|container_network_transmit_packets_total|container_network_receive_packets_total|container_network_transmit_packets_dropped_total|container_network_receive_packets_dropped_total|kube_pod_created|kube_pod_owner|kube_pod_status_reason|node_cpu_seconds_total|node_memory_MemAvailable_bytes|kube_node_role|kube_node_created|node_load1|node_load5|node_load15|node_disk_reads_completed_total|node_disk_writes_completed_total|node_disk_read_bytes_total|node_disk_written_bytes_total|node_disk_read_time_seconds_total|node_disk_write_time_seconds_total|node_network_transmit_packets_total|node_network_receive_packets_total|node_network_transmit_drop_total|node_network_receive_drop_total|kube_replicaset_owner|kube_deployment_created|kube_deployment_status_condition|kube_deployment_spec_replicas|kube_namespace_status_phase|kubelet_volume_stats_inodes|kubelet_volume_stats_inodes_used|kubelet_volume_stats_used_bytes|kube_persistentvolumeclaim_info|kube_persistentvolumeclaim_resource_requests_storage_bytes|kube_job_owner
        # need to also enable the flag: enableMetricsFilter.gke=true
        gke: kube_daemonset_labels|kube_daemonset_status_number_ready|kube_daemonset_status_number_available|kube_daemonset_status_number_unavailable|kube_daemonset_status_current_number_scheduled|kube_daemonset_status_number_misscheduled|kube_daemonset_status_desired_number_scheduled|kube_job_labels|kube_job_complete|kube_job_status_failed|kube_job_status_succeeded|kube_job_complete|kube_job_status_failed|kube_job_status_completion_time|kube_replicaset_labels|kube_replicaset_spec_replicas|kube_replicaset_status_replicas|kube_replicaset_status_ready_replicas|kube_statefulset_replicas|kube_statefulset_status_replicas|kube_statefulset_status_replicas_updated|kube_statefulset_status_replicas_available|kube_pod_container_status_terminated_reason|kube_node_labels|kube_pod_container_status_waiting_reason|node_memory_Buffers_bytes|node_memory_Cached_bytes|kube_deployment_labels|container_cpu_usage_seconds_total|container_memory_working_set_bytes|container_network_receive_bytes_total|container_network_transmit_bytes_total|kube_deployment_status_replicas|kube_deployment_status_replicas_available|kube_deployment_status_replicas_unavailable|kube_deployment_status_replicas_updated|kube_node_info|kube_node_spec_unschedulable|kube_node_status_allocatable|kube_node_status_capacity|kube_node_status_condition|kube_pod_container_resource_requests|kube_pod_container_status_ready|kube_pod_container_status_restarts_total|kube_pod_container_status_running|kube_pod_container_status_terminated|kube_pod_container_status_waiting|kube_pod_info|kube_pod_status_phase|machine_cpu_cores|namespace|node_boot_time_seconds|node_cpu_seconds_total|node_disk_io_time_seconds_total|node_filesystem_avail_bytes|node_filesystem_free_bytes|node_filesystem_size_bytes|node_memory_MemFree_bytes|node_memory_MemTotal_bytes|node_network_receive_bytes_total|node_network_transmit_bytes_total|node_time_seconds|p8s_logzio_name|kube_pod_status_ready|kube_pod_container_status_restarts_total|kube_pod_container_resource_limits|container_memory_usage_bytes|container_network_transmit_packets_total|container_network_receive_packets_total|container_network_transmit_packets_dropped_total|container_network_receive_packets_dropped_total|kube_pod_created|kube_pod_owner|kube_pod_status_reason|node_cpu_seconds_total|node_memory_MemAvailable_bytes|kube_node_role|kube_node_created|node_load1|node_load5|node_load15|node_disk_reads_completed_total|node_disk_writes_completed_total|node_disk_read_bytes_total|node_disk_written_bytes_total|node_disk_read_time_seconds_total|node_disk_write_time_seconds_total|node_network_transmit_packets_total|node_network_receive_packets_total|node_network_transmit_drop_total|node_network_receive_drop_total|kube_replicaset_owner|kube_deployment_created|kube_deployment_status_condition|kube_deployment_spec_replicas|kube_namespace_status_phase|kubelet_volume_stats_inodes|kubelet_volume_stats_inodes_used|kubelet_volume_stats_used_bytes|kube_persistentvolumeclaim_info|kube_persistentvolumeclaim_resource_requests_storage_bytes|kube_job_owner
        custom: 
      drop:
        custom: 
    # for applications pipeline: applications job
    applications:
      keep:
        custom:
      drop:
        custom: 
  
  # Namespaces names to be filtered
  # All values should be listed with | seperator, as regex. i.e: namespace_1|namespace_2|namespace_3
  namespaces:
    # for infrastructure pipeline: metrics/infrastructure & metrics/cadvisor receivers
    # (kubernetes-service-endpoints & cadvisor jobs)
    infrastructure:
      keep:
        custom:
      drop:
        kubeSystem: kube-system # need to also enable the flag: enableMetricsFilter.kubeSystem=true
        custom:
    # for applications pipeline: applications job
    applications:
      keep:
        custom:
      drop:
        custom:
  
  # Services names to filtered
  # All values should be listed with | seperator, as regex. i.e: service_1|service_2|service_3
  services:
    # for infrastructure pipeline: metrics/infrastructure & metrics/cadvisor receivers
    # (kubernetes-service-endpoints & cadvisor jobs)
    infrastructure:
      keep:
        custom: 
      drop:
        kubeDns: kube-dns # need to also enable the flag: disableKubeDnsScraping=true
        custom:




