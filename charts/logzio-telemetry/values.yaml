# Default values for logzio-telemetry.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

collector:
  mode: daemonset # possible values: standalone, daemonset
traces:
  enabled: false # Send traces telemetry
metrics:
  enabled: false # Send metrics telemetry 
spm:
  enabled: false # Send Span metrics, requires `traces.enabled`

applicationMetrics:
  ## Send application metrics, requires `metrics.enabled` and the `prometheus.io/scrape: true` annotaion set.
  enabled: false 
kubeStateMetrics:
  ## If false, kube-state-metrics sub-chart will not be installed
  enabled: true
pushGateway:
  ## If false, prometheus-push-gateway sub-chart will not be installed
  enabled: true
nodeExporter:
  ## If false, prometheus-node-exporter will not be installed
  enabled: true

nameOverride: "otel-collector" # The resources name prefix 
fullnameOverride: "" # The resources name

global:
  # Metrics account shipping token
  logzioMetricsToken: ""
  # Tracing account shipping token
  logzioTracesToken: ""
  # Span Metrics account shipping token
  logzioSpmToken: ""
  # Kubernetes objects Logs account shipping token
  logzioLogsToken: ""
  # Environment name 
  env_id: "my_environment"
  # Traces & Logs listener address region code - https://docs.logz.io/docs/user-guide/admin/hosting-regions/account-region/
  logzioRegion: "us"
  # Custom Traces endpoint, overrides global.LogzioRegion listener address
  customTracesEndpoint: ""
  # Custom Metrics endpoint, overrides global.LogzioRegion listener address
  customMetricsEndpoint: ""
  # Custom Logs endpoint, overrides global.LogzioRegion listener address
  customLogsEndpoint: ""
  # Optional - Identifier for the Kubernetes distribution used. One of "eks", "aks" or "gke".
  distribution: ""
  # Optional - Control enabling of the OpenTelemetry Collector's resource detection feature. Dependent on `distribution` value.
  resourceDetection:
    enabled: false

secrets:
  name: logzio-secret  # Secret name
  enabled: true  # Create secret
  windowsNodeUsername: ""  # Windows Node Username
  windowsNodePassword: ""  # Windows Node Password
  SamplingProbability: 10  # Traces Sampling Probability
  SamplingLatency: 500  # Traces Sampling Latency

# ---
# Filters for Prometheus metrics (infrastructure/applications pipelines)
# 
# NEW SYNTAX (Recommended):
# Use this simpler, more flexible syntax for new deployments:
# filters:
#   infrastructure:
#     exclude:
#       namespace: "kube-system"
#       attribute:
#         deployment.environment: "dev|test"
#   applications:
#     exclude:
#       name: "go_gc_duration_seconds"
#     include:
#       namespace: "prod"
#       attribute:
#         http.status_code: "5*"
#
# LEGACY SYNTAX (Backward Compatibility):
# The prometheusFilters section below is maintained for existing customers.
# For new deployments, prefer the new filters syntax above.
#
# All filter values are regular expressions.
# See README.md for full syntax and mapping.
filters: {}

enableServiceLinks: true

managedServiceAccount: true 

clusterRole:
  # Specifies whether a clusterRole should be created
  create: false
  # Annotations to add to the clusterRole
  annotations: {}
  # The name of the clusterRole to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""
  # A set of rules as documented here : https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  rules: []
  # - apiGroups:
  #   - ''
  #   resources:
  #   - 'pods'
  #   - 'nodes'
  #   verbs:
  #   - 'get'
  #   - 'list'
  #   - 'watch'
  clusterRoleBinding:
    # Annotations to add to the clusterRoleBinding
    annotations: {}
    # The name of the clusterRoleBinding to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""

clusterRoleRules:
- apiGroups:
  - ""
  resources:
  - events
  - namespaces
  - namespaces/status
  - nodes
  - nodes/spec
  - pods
  - pods/metrics
  - nodes/metrics
  - pods/status
  - replicationcontrollers
  - replicationcontrollers/status
  - resourcequotas
  - services
  - endpoints
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - apps
  resources:
  - daemonsets
  - deployments
  - replicasets
  - statefulsets
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - extensions
  resources:
  - daemonsets
  - deployments
  - replicasets
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - batch
  resources:
  - jobs
  - cronjobs
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - autoscaling
  resources:
  - horizontalpodautoscalers
  verbs:
  - get
  - list
  - watch

prometheus-pushgateway:
  serviceAnnotations:
    prometheus.io/scrape: "true"
  nodeSelector:
    kubernetes.io/os: linux
# Configuration merge from the colelctor configs
emptyConfig: {}
# Base configuration of the collector
baseCollectorConfig:
  exporters:
    logging:
      loglevel: info
  extensions:
    health_check: {}
  processors:
    k8sattributes:
      extract:
        metadata:
        - k8s.pod.name
        - k8s.deployment.name
        - k8s.namespace.name
        - k8s.node.name
        - k8s.statefulset.name
        - k8s.replicaset.name
        - k8s.daemonset.name
        - k8s.cronjob.name
        - k8s.job.name
    resource/k8s:
      attributes:
      # Rename
      - key: pod
        action: insert
        from_attribute: k8s.pod.name
      - key: kubernetes_node
        action: insert
        from_attribute: k8s.node.name
      - key: kubernetes_namespace
        action: insert
        from_attribute: k8s.namespace.name
      - key: kubernetes_deployment
        action: insert
        from_attribute: k8s.deployment.name
      - key: kubernetes_pod_ip
        action: insert
        from_attribute: k8s.pod.ip
      - key: kubernetes_statefulset
        action: insert
        from_attribute: k8s.statefulset.name
      - key: kubernetes_replicaset
        action: insert
        from_attribute: k8s.replicaset.name
      - key: kubernetes_cronjob
        action: insert
        from_attribute: k8s.cronjob.name
      - key: kubernetes_daemonset
        action: insert
        from_attribute: k8s.daemonset.name
      - key: kubernetes_job
        action: insert
        from_attribute: k8s.job.name
      # Delete old
      - key: k8s.deployment.name
        action: delete
      - key: k8s.pod.name
        action: delete
      - key: k8s.namespace.name
        action: delete
      - key: k8s.node.name
        action: delete
      - key: k8s.pod.ip
        action: delete
      - key: k8s.statefulset.name
        action: delete
      - key: k8s.replicaset.name
        action: delete
      - key: k8s.daemonset.name
        action: delete
      - key: k8s.job.name
        action: delete
      - key: k8s.cronjob.name
        action: delete
    batch: {}
    attributes/env_id:
      actions:
        - key: env_id
          value: ${ENV_ID}
          action: insert
        - key: logzio_agent_version
          value: ${LOGZIO_AGENT_VERSION}
          action: insert
  service:
    extensions:
      - health_check
    telemetry:
      logs:
        level: "info"
# Traces configuration
tracesConfig:
  exporters:
    logzio:
      endpoint: ${CUSTOM_TRACING_ENDPOINT}
      region: ${LOGZIO_LISTENER_REGION}
      account_token: ${TRACES_TOKEN}
      headers:
        user-agent: "{{ .Chart.Name }}-{{ .Chart.Version }}-helm"
    logging:
      loglevel: info
  extensions:
    pprof:
      endpoint: :1777
    zpages:
      endpoint: :55679
  receivers:
    jaeger:
      protocols:
        thrift_compact:
          endpoint: "0.0.0.0:6831"
        thrift_binary:
          endpoint: "0.0.0.0:6832"
        grpc:
          endpoint: "0.0.0.0:14250"
        thrift_http:
          endpoint: "0.0.0.0:14268"
    otlp:
      protocols:
        grpc:
          endpoint: "0.0.0.0:4317"
        http:
          endpoint: "0.0.0.0:4318"
    zipkin:
      endpoint: "0.0.0.0:9411"
  processors:
    tail_sampling:
      policies:
        [
            {
              name: error-in-policy,
              type: status_code,
              status_code: {status_codes: [ERROR]}
            },
            {
              name: slow-traces-policy,
              type: latency,
              latency: {threshold_ms: "${SAMPLING_LATENCY}" }            
            },
            {
              name: probability-policy,
              type: probabilistic,
              probabilistic: {sampling_percentage: "${SAMPLING_PROBABILITY}" }
            }       
        ]
  service:
    extensions:
      - health_check
      - pprof
      - zpages
    pipelines:
      traces:
        receivers: [jaeger, zipkin, otlp]
        processors: [attributes/env_id, k8sattributes, resource/k8s, tail_sampling, batch]
        exporters: [logzio]

spmForwarderConfig:
  exporters:
    otlp:
      endpoint: "${SPM_SERVICE_ENDPOINT}"
      tls:
        insecure: true
  service:
    pipelines:
      traces/spm:
        receivers: [jaeger, zipkin, otlp]
        processors: [attributes/env_id, k8sattributes, batch]
        exporters: [logging, otlp]
# Metrics standalone collector configuration 
metricsConfig:
  extensions:
    health_check: {}
  processors:
    filter/kubernetes360:
      metrics:
        datapoint:
          - 'IsMatch(metric.name, "(${K8S_360_METRICS})") == true and attributes["logzio_app"] != "kubernetes360"'
    batch: {}
  exporters:
    prometheusremotewrite/applications:
      timeout: 30s
      endpoint: ${LISTENER_URL}
      external_labels:
        p8s_logzio_name: ${ENV_ID}
      headers:
        Authorization: "Bearer ${METRICS_TOKEN}"
        user-agent: "{{ .Chart.Name }}-{{ .Chart.Version }}-helm"
    prometheusremotewrite/infrastructure:
      timeout: 30s
      endpoint: ${LISTENER_URL}
      external_labels:
        p8s_logzio_name: ${ENV_ID}
      headers:
        Authorization: "Bearer ${METRICS_TOKEN}"
        user-agent: "{{ .Chart.Name }}-{{ .Chart.Version }}-helm"
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: "0.0.0.0:4317"
        http:
          endpoint: "0.0.0.0:4318"
    prometheus/applications:
      config:
        global:
          scrape_interval: 60s
          scrape_timeout: 60s
        scrape_configs:
        - job_name: applications
          honor_timestamps: true
          honor_labels: true
          metrics_path: /metrics
          scheme: http
          kubernetes_sd_configs:
          - role: pod
          relabel_configs:
            - action: keep
              regex: true|"true"
              source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            - action: replace
              regex: (https?)
              source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]
              target_label: __scheme__
            - action: replace
              regex: (.+)
              source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
              target_label: __metrics_path__
            - action: replace
              regex: (.+?)(?::\d+)?;(\d+)
              replacement: $$1:$$2
              source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
              target_label: __address__
            - action: labelmap
              regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)
              replacement: __param_$$1
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - action: replace
              source_labels: [__meta_kubernetes_namespace]
              target_label: namespace
            - action: replace
              source_labels: [__meta_kubernetes_pod_name]
              target_label: pod
            - source_labels: [__meta_kubernetes_pod_node_name]
              action: replace
              target_label: kubernetes_node          
          metric_relabel_configs: []
    prometheus/kubelet:
      config:
        global:
          scrape_interval: 30s
          scrape_timeout: 30s
        scrape_configs:
        - authorization:
            credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token
            type: Bearer
          job_name: kubernetes-nodes
          kubernetes_sd_configs:
          - role: node
          metric_relabel_configs: []
          relabel_configs:
          - replacement: kubernetes.default.svc:443
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - action: replace
            replacement: kubernetes360
            target_label: logzio_app            
          - regex: (.+)            
            replacement: /api/v1/nodes/$${1}/proxy/metrics
            source_labels:
            - __meta_kubernetes_node_name
            target_label: __metrics_path__
          - action: replace
            replacement: kubernetes360
            target_label: logzio_app
          scheme: https
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            insecure_skip_verify: true
    prometheus/cadvisor:
      config:
        global:
          scrape_interval: 30s
          scrape_timeout: 30s
        scrape_configs:
        - job_name: 'kubernetes-cadvisor'
          scheme: https
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            insecure_skip_verify: true
          authorization:
            type: Bearer
            credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token
          kubernetes_sd_configs:
          - role: node
          relabel_configs:
          - replacement: kubernetes.default.svc:443
            target_label: __address__        
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - action: replace
            replacement: kubernetes360
            target_label: logzio_app
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/$${1}/proxy/metrics/cadvisor
          - action: replace
            replacement: kubernetes360
            target_label: logzio_app
          metric_relabel_configs: []
    prometheus/collector:
      config:
        global:
          scrape_interval: 30s
          scrape_timeout: 30s
        scrape_configs:
        # Job to collect opentelemetry collector metrics
        - job_name: 'collector-metrics'
          scrape_interval: 15s
          static_configs:
          - targets: [ "0.0.0.0:8888" ]
    prometheus/infrastructure:
      config:
        global:
          scrape_interval: 30s
          scrape_timeout: 30s
        scrape_configs:
        - job_name: windows-metrics
          honor_timestamps: true
          honor_labels: true
          metrics_path: /metrics
          scheme: http
          kubernetes_sd_configs:
          - role: pod
          relabel_configs:
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_windows_io_scrape]
              action: keep
              regex: true|"true"
          metric_relabel_configs: []
        - job_name: kubernetes-service-endpoints
          honor_timestamps: true
          honor_labels: true
          metrics_path: /metrics
          scheme: http
          kubernetes_sd_configs:
          - role: endpoints
          relabel_configs:
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
              action: keep
              regex: true|"true"
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
              action: replace
              target_label: __scheme__
              regex: (https?)
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
              action: replace
              target_label: __address__
              regex: ([^:]+)(?::\d+)?;(\d+)
              replacement: $$1:$$2
            - action: labelmap
              regex: __meta_kubernetes_service_label_(.+)
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: namespace
            - source_labels: [__meta_kubernetes_service_name]
              action: replace
              target_label: service
            - source_labels: [__meta_kubernetes_pod_name]
              action: replace
              target_label: pod
            - source_labels: [__meta_kubernetes_pod_node_name]
              action: replace
              target_label: kubernetes_node
            - source_labels: [__meta_kubernetes_service_annotation_logz_io_app]
              action: replace
              target_label: logzio_app                 
          metric_relabel_configs: []
  service:
    extensions:
      - health_check
    pipelines:
      metrics/infrastructure:
        exporters:
          - prometheusremotewrite/infrastructure
        processors:
          - attributes/env_id
          - filter/kubernetes360
          - batch
        receivers:
          - prometheus/infrastructure
          - prometheus/cadvisor
          - prometheus/kubelet
          - prometheus/collector
          - otlp
          
# Shared params for daemonsetCollector and standaloneCollector deployment pods.
# Can be overridden here or for any component independently using the same keys.

image:
  # If you want to use the contrib image `otel/opentelemetry-collector-contrib`, you also need to change `command.name` value to `otelcontribcol`.
  repository: otel/opentelemetry-collector-contrib
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "0.103.0"
nginxWindowsImage:
  # Reverse proxy image to enable metrics scraping from windows nodes
  repository: logzio/logzio-windows-node-reverse-proxy
  pullPolicy: IfNotPresent
  tag: "0.0.1"
windowsExporterInstallerImage:
  # Job image to install windows exporter on windows nodes
  repository: logzio/logzio-windows-exporter-installer
  pullPolicy: IfNotPresent
  tag: "0.0.1"
spmImage:
  repository: otel/opentelemetry-collector-contrib
  pullPolicy: IfNotPresent
  tag: "0.103.0"
imagePullSecrets: []

# OpenTelemetry Collector executable
command:
  name: otelcol-contrib
  extraArgs: 

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podSecurityContext: {}
securityContext: {}
containerSecurityContext: {}

nodeSelector: {}
linuxNodeSelector: {
  "kubernetes.io/os": linux
}

tolerations: []

affinity: {}

## Topology spread constraints for pod assignment
## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
topologySpreadConstraints: []

# Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
podDisruptionBudget:
  enabled: false
# Set the minimum available replicas
#  minAvailable: 1
# OR Set the maximum unavailable replicas
#  maxUnavailable: 1
# If both defined only maxUnavailable will be used

extraEnvs: []
extraConfigMapMounts: []
extraHostPathMounts: []
secretMounts: []

# PriorityClass name for the collector pods
priorityClassName: ""

# Configuration for ports, shared between daemonsetCollector, standaloneCollector and service.
# Can be overridden here or for daemonsetCollector and standaloneCollector independently.
ports:
  otlp:
    enabled: true
    containerPort: 4317
    servicePort: 4317
    hostPort: 4317
    protocol: TCP
  otlp-http:
    enabled: true
    containerPort: 4318
    servicePort: 4318
    hostPort: 4318
    protocol: TCP
  jaeger-compact:
    enabled: true
    containerPort: 6831
    servicePort: 6831
    hostPort: 6831
    protocol: UDP
  jaeger-binary:
    enabled: true
    containerPort: 6832
    servicePort: 6832
    hostPort: 6832
    protocol: UDP    
  jaeger-thrift:
    enabled: true
    containerPort: 14268
    servicePort: 14268
    hostPort: 14268
    protocol: TCP
  jaeger-grpc:
    enabled: true
    containerPort: 14250
    servicePort: 14250
    hostPort: 14250
    protocol: TCP
  zipkin:
    enabled: true
    containerPort: 9411
    servicePort: 9411
    hostPort: 9411
    protocol: TCP
  metrics:
    enabled: true
    containerPort: 8888
    servicePort: 8888
    hostPort: 8888
    protocol: TCP
  signalfx:
    enabled: false
    containerPort: 9943
    servicePort: 9943
    hostPort: 9943
    protocol: TCP
  carbon:
    enabled: false
    containerPort: 2003
    servicePort: 2003
    hostPort: 2003
    protocol: TCP

# SignalFx config
signalFx:
  enabled: false
  config:
    receivers:
      # SignalFx receiver to accept metrics from SignalFx client libraries # https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/signalfxreceiver/README.md
      signalfx:
        endpoint: "0.0.0.0:9943"
        access_token_passthrough: true
    processors:
      # Batch processor to improve performance by grouping metrics
      batch/signalfx: {}
    exporters:
      # Prometheus Remote Write exporter for Logz.io
      prometheusremotewrite/signalfx:
        endpoint: "${LISTENER_URL}"
        headers:
          Authorization: "Bearer ${METRICS_TOKEN}"
          user-agent: '{{ .Chart.Name }}-{{ .Chart.Version }}-helm'          
        external_labels:
          p8s_logzio_name: ${ENV_ID}

    service:
      pipelines:
        metrics/signalfx:
          receivers: [signalfx]
          processors: [batch/signalfx]
          exporters: [prometheusremotewrite/signalfx]

# Carbon config
carbon:
  enabled: false
  config:
    receivers:
      # Carbon receiver to accept metrics from Carbon/Graphite client libraries # https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/carbonreceiver/README.md
      carbon:
        endpoint: "0.0.0.0:2003"
        transport: tcp
    processors:
      # Batch processor to improve performance by grouping metrics
      batch/carbon: {}
    exporters:
      # Prometheus Remote Write exporter for Logz.io
      prometheusremotewrite/carbon:
        endpoint: "${LISTENER_URL}"
        headers:
          Authorization: "Bearer ${METRICS_TOKEN}"
          user-agent: '{{ .Chart.Name }}-{{ .Chart.Version }}-helm'          
        external_labels:
          p8s_logzio_name: ${ENV_ID}

    service:
      pipelines:
        metrics/carbon:
          receivers: [carbon]
          processors: [batch/carbon]
          exporters: [prometheusremotewrite/carbon]

# Kubernetes Object logs
k8sObjectsConfig:
  enabled: false
  config:
    receivers:
      # Watch for changes in Kubernetes objects
      k8sobjects/watch:
        objects:
          - name: pods
            mode: watch
            exclude_watch_type: [ DELETED, BOOKMARK ]
          - name: deployments
            mode: watch
            exclude_watch_type: [ DELETED, BOOKMARK ]
          - name: daemonsets
            mode: watch
            exclude_watch_type: [ DELETED, BOOKMARK ]
          - name: statefulsets
            mode: watch
            exclude_watch_type: [ DELETED, BOOKMARK ]
          - name: jobs
            mode: watch
            exclude_watch_type: [ DELETED, BOOKMARK ]
          - name: nodes
            mode: watch
            exclude_watch_type: [ DELETED, BOOKMARK ]
      # Pull Kubernetes objects every 3 hours(default)
      k8sobjects/pull:
        objects:
          - name: pods
            mode: pull
            interval: 180m
          - name: deployments
            mode: pull
            interval: 180m
          - name: daemonsets
            mode: pull
            interval: 180m
          - name: statefulsets
            mode: pull
            interval: 180m
          - name: jobs
            mode: pull
            interval: 180m
          - name: nodes
            mode: pull
            interval: 180m
    processors:
      # Adds eventType key with value of type key, then sets type to k8s_object
      transform/log_type:
        error_mode: ignore
        log_statements:
          - context: log
            statements:
              - set(body["eventType"],body["type"]) where body["type"] != "k8s_object"
              - set(body["type"], "k8s_object")
      resource/env_id:
        attributes:
        # Adds env_id key with value from the secret
        - key: env_id
          action: insert
          value: ${ENV_ID}
      # For pulled objects, copy the log into object key       
      transform/pulled_object:
        error_mode: ignore
        log_statements:
          - context: log
            statements:
              - set(body["object"],body) where body["object"] == nil
              - keep_keys(body, ["object", "type", "eventType"])
      # Remove managed fields metadata key              
      transform/remove_managedfields:
        error_mode: ignore
        log_statements:
          - context: log
            statements:
              - delete_key(body["object"]["metadata"], "managedFields")
      batch/k8sobjects: {}
    exporters:
      logzio/object_logs:
        account_token: "${OBJECTS_LOGS_TOKEN}"
        region: "${LOGZIO_LISTENER_REGION}"
        endpoint: "${CUSTOM_LOGS_ENDPOINT}"
        headers:
          user-agent: "{{ .Chart.Name }}-{{ .Chart.Version }}-helm"
    service:
      pipelines:
        logs/k8sobjects:
          receivers: [k8sobjects/pull,k8sobjects/watch]
          processors: [transform/pulled_object,transform/remove_managedfields,transform/log_type, resource/env_id, batch/k8sobjects]
          exporters: [logzio/object_logs]
# Service graph metrics configuration
serviceGraph:
  enabled: true
  config:
    connectors:
      servicegraph:
        latency_histogram_buckets: [2ms, 8ms, 50ms, 100ms, 200ms, 500ms, 1s, 5s, 10s]
        dimensions:
          - env_id
        store:
          ttl: 5s
          max_items: 100000
        metrics_flush_interval: 60s
    service:
      pipelines:
        traces:
          exporters: [servicegraph]      
        metrics/spm-logzio:
          receivers: [servicegraph]
# Span metrics configuration
spanMetricsAgregator:
  config:
    processors:
      metricstransform/metrics-rename:
        transforms:
        - include: ^duration(.*)$$
          action: update
          match_type: regexp
          new_name: latency.$${1} 
        - action: update
          include: calls
          new_name: calls_total
      metricstransform/labels-rename:
        transforms:
        - action: update
          include: ^latency
          match_type: regexp
          operations:
          - action: update_label
            label: span.name
            new_label: operation
        - action: update
          include: ^calls
          match_type: regexp
          operations:
          - action: update_label
            label: span.name
            new_label: operation  
      batch/spanmetrics: {}
    connectors:
      spanmetrics:
        aggregation_temporality: AGGREGATION_TEMPORALITY_CUMULATIVE
        dimensions:
        - name: rpc.grpc.status_code
        - name: http.method
        - name: http.status_code
        - name: k8s.pod.name
        - name: k8s.deployment.name
        - name: k8s.namespace.name
        - name: k8s.node.name
        - name: k8s.statefulset.name
        - name: k8s.replicaset.name
        - name: k8s.daemonset.name
        - name: k8s.cronjob.name
        - name: k8s.job.name
        - name: cloud.provider
        - name: cloud.region
        - name: db.system
        - name: messaging.system
        - default: ${ENV_ID}
          name: env_id
        dimensions_cache_size: 100000
        histogram:
          explicit:
            buckets:
            - 2ms
            - 8ms
            - 50ms
            - 100ms
            - 200ms
            - 500ms
            - 1s
            - 5s
            - 10s
        metrics_expiration: 5m
        resource_metrics_key_attributes:
        - service.name
        - telemetry.sdk.language
        - telemetry.sdk.name
    exporters:
      logging:
        loglevel: info
      prometheus/spm:
        endpoint: 0.0.0.0:8889
      prometheusremotewrite/spm-logzio:
        endpoint: ${LISTENER_URL}
        headers:
          Authorization: Bearer ${SPM_TOKEN}
          user-agent: "{{ .Chart.Name }}-{{ .Chart.Version }}-helm"
        timeout: 30s
        add_metric_suffixes: false
    receivers:
      jaeger:
        protocols:
          grpc:
            endpoint: "0.0.0.0:14250"
          thrift_binary:
            endpoint: "0.0.0.0:6832"
          thrift_compact:
            endpoint: "0.0.0.0:6831"
          thrift_http:
            endpoint: "0.0.0.0:14268"
      otlp:
        protocols:
          grpc:
            endpoint: "0.0.0.0:4317"
          http:
            endpoint: "0.0.0.0:4318"
      zipkin:
        endpoint: "0.0.0.0:9411"
    service:
      pipelines:
        metrics/spm-logzio:
          exporters:
          - prometheusremotewrite/spm-logzio
          - prometheus/spm
          processors:
          - metricstransform/metrics-rename
          - metricstransform/labels-rename
          - batch/spanmetrics
          receivers:
          - spanmetrics
        traces:
          exporters:
          - logging
          - spanmetrics
          receivers:
          - jaeger
          - zipkin
          - otlp
      telemetry:
        logs:
          level: "info"

  # service values        
  service:
    type: ClusterIP
    annotations: {}
  ports:
    otlp:
      enabled: true
      containerPort: 4317
      servicePort: 4317
      hostPort: 4317
      protocol: TCP
    otlp-http:
      enabled: true
      containerPort: 4318
      servicePort: 4318
      hostPort: 4318
      protocol: TCP
    jaeger-compact:
      enabled: true
      containerPort: 6831
      servicePort: 6831
      hostPort: 6831
      protocol: UDP
    jaeger-binary:
      enabled: true
      containerPort: 6832
      servicePort: 6832
      hostPort: 6832
      protocol: UDP       
    jaeger-thrift:
      enabled: true
      containerPort: 14268
      servicePort: 14268
      hostPort: 14268
      protocol: TCP
    jaeger-grpc:
      enabled: true
      containerPort: 14250
      servicePort: 14250
      hostPort: 14250
      protocol: TCP
    zipkin:
      enabled: true
      containerPort: 9411
      servicePort: 9411
      hostPort: 9411
      protocol: TCP


  resources: {}
   ## Resources are not specified by default, to ensure the chart runs on environments with little resources.
   ## If you want to specify resources, uncomment the following  lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 256m
    #   memory: 512Mi
    # requests:
    #   cpu: 512m
    #   memory: 1024Mi

# Configuration for standalone OpenTelemetry Collector deployment, enabled by default
standaloneCollector:
  enabled: false

  containerLogs:
    enabled: false

  resources: {}
   ## Resources are not specified by default, to ensure the chart runs on environments with little resources.
   ## If you want to specify resources, uncomment the following  lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 200m
    #   memory: 512Mi
    # requests:
    #   cpu: 500m
    #   memory: 1024Mi
  podLabels:  {}

  podAnnotations: {}
  
  # Configuration override that will be merged into the collector default config
  configOverride: {}

  replicaCount: 1
  
daemonsetCollector:
  enabled: false

  containerLogs:
    enabled: false
  
  # prevent collector daemonset deployment on fargate nodes
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: eks.amazonaws.com/compute-type
                operator: DoesNotExist


  resources: {}
   ## Resources are not specified by default, to ensure the chart runs on environments with little resources.
   ## If you want to specify resources, uncomment the following  lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 150m
    #   memory: 256Mi
    # requests:
    #   cpu: 300m
    #   memory: 512Mi
  podLabels:  {}

  podAnnotations: {}
  
  # Configuration override that will be merged into the daemonset default config
  configOverride: {}

service:
  type: ClusterIP
  clusterIP: ""
  annotations: {}

# autoscaling is used only if standaloneCollector enabled
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

windowsExporterInstallerJob:
  interval: "*/10 * * * *"
  concurrencyPolicy: Forbid            # Future cronjob will run only after current job is finished
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  ttlSecondsAfterFinished: 3600        # First job only (Not CronJob)   

podLabels:  {}
annotations:  {}

kube-state-metrics:
  service:
    annotations:
      logz.io/app: "kubernetes360"
  nodeSelector:
    kubernetes.io/os: linux
  podSecurityPolicy:
    enabled: false
  extraArgs: ["--metric-labels-allowlist=nodes=[kubernetes.azure.com/scalesetpriority,eks.amazonaws.com/capacityType,cloud.google.com/gke-preemptible]"]

prometheus-node-exporter:
  service:
    port: 9101
    targetPort: 9101
    annotations:
      prometheus.io/scrape: "true"
      logz.io/app: "kubernetes360"
  nodeSelector:
    kubernetes.io/os: linux
  # Prevent node exporter deamonset deploymment on fargate nodes
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: eks.amazonaws.com/compute-type
                operator: DoesNotExist
  rbac:
    pspEnabled: false
# Filter only metrics relevant for prebuilt content
enableMetricsFilter:
  gke: false  # Google Kubernetes Engine
  eks: false  # Amazon Elastic Kubernetes Service
  aks: false  # Azure Kubernetes Service
  dropKubeSystem: false  # Drop kube-system metrics

disableKubeDnsScraping: false  # Kube DNS metrics scraping

# Metrics Daemonset collector configuration
daemonsetConfig:
  extensions:
    health_check: {}
  processors:
    filter/kubernetes360:
      metrics:
        datapoint:
          - 'IsMatch(metric.name, "(${K8S_360_METRICS})") == true and attributes["logzio_app"] != "kubernetes360"'
          # Workaround for an issue where metrics are scraped multiple times
          - 'attributes["job_dummy"] == "kubernetes-service-endpoints" and attributes["kubernetes_node"] == nil'
    # Removes label needed for duplicate metrics checks
    attributes/remove_job_dummy:
      actions:
        - key: job_dummy
          action: delete
    batch: {}
  exporters:
    prometheusremotewrite/applications:
      timeout: 30s
      endpoint: ${LISTENER_URL}
      external_labels:
        p8s_logzio_name: ${ENV_ID}
      headers:
        Authorization: "Bearer ${METRICS_TOKEN}"
        user-agent: "{{ .Chart.Name }}-{{ .Chart.Version }}-helm"
    prometheusremotewrite/infrastructure:
      timeout: 30s
      endpoint: ${LISTENER_URL}
      external_labels:
        p8s_logzio_name: ${ENV_ID}
      headers:
        Authorization: "Bearer ${METRICS_TOKEN}"
        user-agent: "{{ .Chart.Name }}-{{ .Chart.Version }}-helm"
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: "0.0.0.0:4317"
        http:
          endpoint: "0.0.0.0:4318"
    prometheus/applications:
      config:
        global:
          scrape_interval: 30s
          scrape_timeout: 30s
        scrape_configs:
        - job_name: applications
          honor_timestamps: true
          honor_labels: true
          metrics_path: /metrics
          scheme: http
          kubernetes_sd_configs:
          - role: pod
            selectors:
            # only scrape data from pods running on the same node as the collector
            - role: pod
              field: "spec.nodeName=$KUBE_NODE_NAME"
          relabel_configs:
            - action: keep
              regex: true|"true"
              source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            - action: replace
              regex: (https?)
              source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]
              target_label: __scheme__
            - action: replace
              regex: (.+)
              source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
              target_label: __metrics_path__
            - action: replace
              regex: (.+?)(?::\d+)?;(\d+)
              replacement: $$1:$$2
              source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
              target_label: __address__
            - action: labelmap
              regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)
              replacement: __param_$$1
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - action: replace
              source_labels: [__meta_kubernetes_namespace]
              target_label: namespace
            - action: replace
              source_labels: [__meta_kubernetes_pod_name]
              target_label: pod
            - source_labels: [__meta_kubernetes_pod_node_name]
              action: replace
              target_label: kubernetes_node          
          metric_relabel_configs: []
    prometheus/kubelet:
      config:
        global:
          scrape_interval: 30s
          scrape_timeout: 30s
        scrape_configs:
        - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
          job_name: kubernetes-nodes
          kubernetes_sd_configs:
          - role: node
            selectors:
            - field: metadata.name=$KUBE_NODE_NAME
              role: node
          metric_relabel_configs: []
          metrics_path: /metrics
          relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - action: replace
            replacement: kubernetes360
            target_label: logzio_app
          scheme: https
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            insecure_skip_verify: true
    prometheus/cadvisor:
      config:
        global:
          scrape_interval: 30s
          scrape_timeout: 30s
        scrape_configs:
        - job_name: 'kubernetes-cadvisor'
          scheme: https
          metrics_path: /metrics/cadvisor
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            insecure_skip_verify: true
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
          kubernetes_sd_configs:
          - role: node
            selectors:
            # only scrape data from node with the same name as the node the collector run on
            - role: node
              field: "metadata.name=$KUBE_NODE_NAME"
          relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+) 
          - action: replace
            replacement: kubernetes360
            target_label: logzio_app            
          metric_relabel_configs: []
    prometheus/collector:
      config:
        global:
          scrape_interval: 30s
          scrape_timeout: 30s
        scrape_configs:
        # Job to collect opentelemetry collector metrics
        - job_name: 'collector-metrics'
          scrape_interval: 15s
          static_configs:
          - targets: [ "0.0.0.0:8888" ] 
          metric_relabel_configs: []
    prometheus/infrastructure:
      config:
        global:
          scrape_interval: 30s
          scrape_timeout: 30s
        scrape_configs:
        - job_name: windows-metrics
          honor_timestamps: true
          honor_labels: true
          metrics_path: /metrics
          scheme: http
          kubernetes_sd_configs:
          - role: pod
            selectors:
            # only scrape data from pods running on the same node as collector
            - role: pod
              field: "spec.nodeName=$KUBE_NODE_NAME"
          relabel_configs:
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_windows_io_scrape]
              action: keep
              regex: true|"true"
          metric_relabel_configs: []
        # Job to collect metrics from applications running on pods
        - job_name: kubernetes-service-endpoints
          honor_timestamps: true
          honor_labels: true
          metrics_path: /metrics
          scheme: http
          kubernetes_sd_configs:
          - role: endpoints
            selectors:
            # only scrape data from pods running on the same node as collector
            - role: pod
              field: "spec.nodeName=$KUBE_NODE_NAME"
          relabel_configs:
            # Adding a dummy job label to enable filtering for duplicate metrics using daemonset collector
            # "job" label is only added in the prometheusremotewrite and cannot be added before
            - action: replace
              replacement: kubernetes-service-endpoints
              target_label: job_dummy
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
              action: keep
              regex: true|"true"
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
              action: replace
              target_label: __scheme__
              regex: (https?)
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
              action: replace
              target_label: __address__
              regex: ([^:]+)(?::\d+)?;(\d+)
              replacement: $$1:$$2
            - action: labelmap
              regex: __meta_kubernetes_service_label_(.+)
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: namespace
            - source_labels: [__meta_kubernetes_service_name]
              action: replace
              target_label: service
            - source_labels: [__meta_kubernetes_pod_name]
              action: replace
              target_label: pod
            - source_labels: [__meta_kubernetes_pod_node_name]
              action: replace
              target_label: kubernetes_node
            - source_labels: [__meta_kubernetes_service_annotation_logz_io_app]
              action: replace
              target_label: logzio_app                 
          metric_relabel_configs: []
  service:
    extensions:
      - health_check
    pipelines:
      metrics/infrastructure:
        exporters:
          - prometheusremotewrite/infrastructure
        processors:
          - attributes/env_id
          - filter/kubernetes360
          # workaround for duplicate metrics sent from kubernetes-service-endpoints job
          - attributes/remove_job_dummy
          - batch
        receivers:
          - prometheus/infrastructure
          - prometheus/cadvisor
          - prometheus/kubelet
          - prometheus/collector
          - otlp
    telemetry:
      logs:
        level: "info" 

         
opencost:
  enabled: false
  config:
    processors:
    # opencost collects duplicates metrics from kube-state and cadvisor.
      filter/opencost-exporter: 
        metrics:
          datapoint:     
            - 'IsMatch(metric.name, "(${OPENCOST_DUPLICATES})") == true and attributes["app"] == "opencost"'
    service:
      pipelines:
        metrics/infrastructure:
          processors:
            - filter/opencost-exporter

# ---
# PROMETHEUS FILTERS (Backward Compatibility)
# This section is maintained for existing customers using the prometheusFilters syntax.
# For new deployments, prefer the new 'filters' syntax above.
# 
# The prometheusFilters section provides complex filtering capabilities for:
# - Metrics filtering (keep/drop) with cloud provider presets (aks/eks/gke)
# - Namespace filtering with kube-system exclusion
# - Service filtering with kube-dns exclusion
# 
# All values should be listed with | separator, as regex. i.e: metric_1|metric_2|metric_3
prometheusFilters:
  # Metrics names to be filtered
  # All values should be listed with | seperator, as regex. i.e: metric_1|metric_2|metric_3
  metrics:
    # for infrastructure pipeline: metrics/infrastructure & metrics/cadvisor receivers
    # (kubernetes-service-endpoints & cadvisor jobs)
    infrastructure:
      keep:
        # need to also enable the flag: enableMetricsFilter.aks=true
        aks: kube_daemonset_labels|kube_daemonset_status_number_ready|kube_daemonset_status_number_available|kube_daemonset_status_number_unavailable|kube_daemonset_status_current_number_scheduled|kube_daemonset_status_number_misscheduled|kube_daemonset_status_desired_number_scheduled|kube_job_labels|kube_job_complete|kube_job_status_failed|kube_job_status_succeeded|kube_job_complete|kube_job_status_failed|kube_job_status_completion_time|kube_replicaset_labels|kube_replicaset_spec_replicas|kube_replicaset_status_replicas|kube_replicaset_status_ready_replicas|kube_statefulset_replicas|kube_statefulset_status_replicas|kube_statefulset_status_replicas_updated|kube_statefulset_status_replicas_available|kube_pod_container_status_terminated_reason|kube_node_labels|kube_pod_container_status_waiting_reason|node_memory_Buffers_bytes|node_memory_Cached_bytes|kube_deployment_labels|container_cpu_usage_seconds_total|container_memory_working_set_bytes|container_network_receive_bytes_total|container_network_transmit_bytes_total|i:|kube_deployment_status_replicas|kube_deployment_status_replicas_available|kube_deployment_status_replicas_unavailable|kube_deployment_status_replicas_updated|kube_node_info|kube_node_spec_unschedulable|kube_node_status_allocatable|kube_node_status_capacity|kube_node_status_condition|kube_pod_container_info|kube_pod_container_resource_requests|kube_pod_container_resource_requests_cpu_cores|kube_pod_container_resource_requests_memory_bytes|kube_pod_container_status_ready|kube_pod_container_status_restarts_total|kube_pod_container_status_running|kube_pod_container_status_terminated|kube_pod_container_status_waiting|kube_pod_info|kube_pod_status_phase|machine_cpu_cores|namespace|node_boot_time_seconds|node_cpu_seconds_total|node_disk_io_time_seconds_total|node_filesystem_avail_bytes|node_filesystem_free_bytes|node_filesystem_size_bytes|node_memory_MemFree_bytes|node_memory_MemTotal_bytes|node_network_receive_bytes_total|node_network_transmit_bytes_total|node_time_seconds|p8s_logzio_name|windows_container_cpu_usage_seconds_total|windows_container_memory_usage_commit_bytes|windows_container_network_receive_bytes_total|windows_container_network_transmit_bytes_total|windows_cpu_time_total|windows_cs_hostname|windows_cs_physical_memory_bytes|windows_logical_disk_free_bytes|windows_logical_disk_read_seconds_total|windows_logical_disk_size_bytes|windows_logical_disk_write_seconds_total|windows_net_bytes_received_total|windows_net_bytes_sent_total|windows_os_physical_memory_free_bytes|windows_system_system_up_time|kube_pod_status_ready|kube_pod_container_status_restarts_total|kube_pod_container_resource_limits|container_memory_usage_bytes|container_network_transmit_packets_total|container_network_receive_packets_total|container_network_transmit_packets_dropped_total|container_network_receive_packets_dropped_total|kube_pod_created|kube_pod_owner|kube_pod_status_reason|node_cpu_seconds_total|node_memory_MemAvailable_bytes|kube_node_role|kube_node_created|node_load1|node_load5|node_load15|node_disk_reads_completed_total|node_disk_writes_completed_total|node_disk_read_bytes_total|node_disk_written_bytes_total|node_disk_read_time_seconds_total|node_disk_write_time_seconds_total|node_network_transmit_packets_total|node_network_receive_packets_total|node_network_transmit_drop_total|node_network_receive_drop_total|kube_replicaset_owner|kube_deployment_created|kube_deployment_status_condition|kube_deployment_spec_replicas|kube_namespace_status_phase|kubelet_volume_stats_inodes|kubelet_volume_stats_inodes_used|kubelet_volume_stats_used_bytes|kube_persistentvolumeclaim_info|kube_persistentvolumeclaim_resource_requests_storage_bytes|kube_job_owner|container_cpu_cfs_throttled_seconds_total|kube_daemonset_status_observed_generation|kube_deployment_status_observed_generation|kube_statefulset_status_observed_generation|kube_statefulset_metadata_generation|kube_deployment_metadata_generation|kube_daemonset_metadata_generation
        # need to also enable the flag: enableMetricsFilter.eks=true
        eks: kube_daemonset_labels|kube_daemonset_status_number_ready|kube_daemonset_status_number_available|kube_daemonset_status_number_unavailable|kube_daemonset_status_current_number_scheduled|kube_daemonset_status_number_misscheduled|kube_daemonset_status_desired_number_scheduled|kube_job_labels|kube_job_complete|kube_job_status_failed|kube_job_status_succeeded|kube_job_complete|kube_job_status_failed|kube_job_status_completion_time|kube_replicaset_labels|kube_replicaset_spec_replicas|kube_replicaset_status_replicas|kube_replicaset_status_ready_replicas|kube_statefulset_replicas|kube_statefulset_status_replicas|kube_statefulset_status_replicas_updated|kube_statefulset_status_replicas_available|kube_pod_container_status_terminated_reason|kube_node_labels|kube_pod_container_status_waiting_reason|node_memory_Buffers_bytes|node_memory_Cached_bytes|kube_deployment_labels|container_cpu_usage_seconds_total|container_memory_working_set_bytes|container_network_receive_bytes_total|container_network_transmit_bytes_total|kube_deployment_status_replicas|kube_deployment_status_replicas_available|kube_deployment_status_replicas_unavailable|kube_deployment_status_replicas_updated|kube_node_info|kube_node_spec_unschedulable|kube_node_status_allocatable|kube_node_status_capacity|kube_node_status_condition|kube_pod_container_resource_requests|kube_pod_container_status_ready|kube_pod_container_status_restarts_total|kube_pod_container_status_running|kube_pod_container_status_terminated|kube_pod_container_status_waiting|kube_pod_info|kube_pod_status_phase|machine_cpu_cores|namespace|node_boot_time_seconds|node_cpu_seconds_total|node_disk_io_time_seconds_total|node_filesystem_avail_bytes|node_filesystem_free_bytes|node_filesystem_size_bytes|node_memory_MemFree_bytes|node_memory_MemTotal_bytes|node_network_receive_bytes_total|node_network_transmit_bytes_total|node_time_seconds|p8s_logzio_name|kube_pod_status_ready|kube_pod_container_status_restarts_total|kube_pod_container_resource_limits|container_memory_usage_bytes|container_network_transmit_packets_total|container_network_receive_packets_total|container_network_transmit_packets_dropped_total|container_network_receive_packets_dropped_total|kube_pod_created|kube_pod_owner|kube_pod_status_reason|node_cpu_seconds_total|node_memory_MemAvailable_bytes|kube_node_role|kube_node_created|node_load1|node_load5|node_load15|node_disk_reads_completed_total|node_disk_writes_completed_total|node_disk_read_bytes_total|node_disk_written_bytes_total|node_disk_read_time_seconds_total|node_disk_write_time_seconds_total|node_network_transmit_packets_total|node_network_receive_packets_total|node_network_transmit_drop_total|node_network_receive_drop_total|kube_replicaset_owner|kube_deployment_created|kube_deployment_status_condition|kube_deployment_spec_replicas|kube_namespace_status_phase|kubelet_volume_stats_inodes|kubelet_volume_stats_inodes_used|kubelet_volume_stats_used_bytes|kube_persistentvolumeclaim_info|kube_persistentvolumeclaim_resource_requests_storage_bytes|kube_job_owner|kube_pod_container_info|container_cpu_cfs_throttled_seconds_total|kube_daemonset_status_observed_generation|kube_deployment_status_observed_generation|kube_statefulset_status_observed_generation|kube_statefulset_metadata_generation|kube_deployment_metadata_generation|kube_daemonset_metadata_generation
        # need to also enable the flag: enableMetricsFilter.gke=true
        gke: kube_daemonset_labels|kube_daemonset_status_number_ready|kube_daemonset_status_number_available|kube_daemonset_status_number_unavailable|kube_daemonset_status_current_number_scheduled|kube_daemonset_status_number_misscheduled|kube_daemonset_status_desired_number_scheduled|kube_job_labels|kube_job_complete|kube_job_status_failed|kube_job_status_succeeded|kube_job_complete|kube_job_status_failed|kube_job_status_completion_time|kube_replicaset_labels|kube_replicaset_spec_replicas|kube_replicaset_status_replicas|kube_replicaset_status_ready_replicas|kube_statefulset_replicas|kube_statefulset_status_replicas|kube_statefulset_status_replicas_updated|kube_statefulset_status_replicas_available|kube_pod_container_status_terminated_reason|kube_node_labels|kube_pod_container_status_waiting_reason|node_memory_Buffers_bytes|node_memory_Cached_bytes|kube_deployment_labels|container_cpu_usage_seconds_total|container_memory_working_set_bytes|container_network_receive_bytes_total|container_network_transmit_bytes_total|kube_deployment_status_replicas|kube_deployment_status_replicas_available|kube_deployment_status_replicas_unavailable|kube_deployment_status_replicas_updated|kube_node_info|kube_node_spec_unschedulable|kube_node_status_allocatable|kube_node_status_capacity|kube_node_status_condition|kube_pod_container_resource_requests|kube_pod_container_status_ready|kube_pod_container_status_restarts_total|kube_pod_container_status_running|kube_pod_container_status_terminated|kube_pod_container_status_waiting|kube_pod_info|kube_pod_status_phase|machine_cpu_cores|namespace|node_boot_time_seconds|node_cpu_seconds_total|node_disk_io_time_seconds_total|node_filesystem_avail_bytes|node_filesystem_free_bytes|node_filesystem_size_bytes|node_memory_MemFree_bytes|node_memory_MemTotal_bytes|node_network_receive_bytes_total|node_network_transmit_bytes_total|node_time_seconds|p8s_logzio_name|kube_pod_status_ready|kube_pod_container_status_restarts_total|kube_pod_container_resource_limits|container_memory_usage_bytes|container_network_transmit_packets_total|container_network_receive_packets_total|container_network_transmit_packets_dropped_total|container_network_receive_packets_dropped_total|kube_pod_created|kube_pod_owner|kube_pod_status_reason|node_cpu_seconds_total|node_memory_MemAvailable_bytes|kube_node_role|kube_node_created|node_load1|node_load5|node_load15|node_disk_reads_completed_total|node_disk_writes_completed_total|node_disk_read_bytes_total|node_disk_written_bytes_total|node_disk_read_time_seconds_total|node_disk_write_time_seconds_total|node_network_transmit_packets_total|node_network_receive_packets_total|node_network_transmit_drop_total|node_network_receive_drop_total|kube_replicaset_owner|kube_deployment_created|kube_deployment_status_condition|kube_deployment_spec_replicas|kube_namespace_status_phase|kubelet_volume_stats_inodes|kubelet_volume_stats_inodes_used|kubelet_volume_stats_used_bytes|kube_persistentvolumeclaim_info|kube_persistentvolumeclaim_resource_requests_storage_bytes|kube_job_owner|kube_pod_container_info|container_cpu_cfs_throttled_seconds_total|kube_daemonset_status_observed_generation|kube_deployment_status_observed_generation|kube_statefulset_status_observed_generation|kube_statefulset_metadata_generation|kube_deployment_metadata_generation|kube_daemonset_metadata_generation
        custom: 
      drop:
        custom: 
    # for applications pipeline: applications job
    applications:
      keep:
        custom:
      drop:
        custom: 
  
  # Namespaces names to be filtered
  # All values should be listed with | seperator, as regex. i.e: namespace_1|namespace_2|namespace_3
  namespaces:
    # for infrastructure pipeline: metrics/infrastructure & metrics/cadvisor receivers
    # (kubernetes-service-endpoints & cadvisor jobs)
    infrastructure:
      keep:
        custom:
      drop:
        kubeSystem: kube-system # need to also enable the flag: enableMetricsFilter.kubeSystem=true
        custom:
    # for applications pipeline: applications job
    applications:
      keep:
        custom:
      drop:
        custom:
  
  # Services names to filtered
  # All values should be listed with | seperator, as regex. i.e: service_1|service_2|service_3
  services:
    # for infrastructure pipeline: metrics/infrastructure & metrics/cadvisor receivers
    # (kubernetes-service-endpoints & cadvisor jobs)
    infrastructure:
      keep:
        custom: 
      drop:
        kubeDns: kube-dns # need to also enable the flag: disableKubeDnsScraping=true
        custom:
