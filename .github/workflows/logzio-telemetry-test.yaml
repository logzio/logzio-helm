name: Test `logzio-telemetry` chart
permissions:
  contents: read
on:
  pull_request:
    branches:
      - master
    paths:
      - 'charts/logzio-telemetry/templates/**'
      - 'charts/logzio-telemetry/Chart.yaml'
      - 'charts/logzio-telemetry/values.yaml'
      - 'charts/logzio-telemetry/windows_exporter_installer/**'
      - '!**.md'
      - '!**/**/**.md'
      - '!**/**/**/**.md'
jobs:
  test-helm-chart:
    name: Test Helm Chart on Kind
    runs-on: ubuntu-latest
    strategy:
      matrix:
        mode: ['daemonset', 'standalone']
    steps:
      - name: Generate random id
        id: random_id
        run: echo "rand=$(echo $RANDOM)" >> $GITHUB_OUTPUT

      - name: Set ENV_ID
        run: echo "ENV_ID=telemetry-test-run-${{ steps.random_id.outputs.rand }}-${{ matrix.mode }}" >> $GITHUB_ENV
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v6
        with:
          go-version: '1.20'

      - name: Set up Helm
        uses: azure/setup-helm@v4.3.0

      - name: Set up kubectl
        uses: azure/setup-kubectl@v4

      - name: Install Kind
        run: |
          curl -Lo ./kind "https://kind.sigs.k8s.io/dl/v0.26.0/kind-Linux-amd64"
          chmod +x ./kind
          sudo mv ./kind /usr/local/bin/kind

      - name: Create Kind cluster
        run: |
          kind create cluster --name kind-${{ github.run_id }}-${{ matrix.mode }}
          kubectl cluster-info

      - name: Debug cluster setup
        run: |
          echo "=== Cluster Information ==="
          kubectl version --client --output=yaml
          kubectl version --output=yaml || echo "Server version not available yet"
          kubectl get nodes -o wide
          kubectl get namespaces
          echo "=== Node Details ==="
          kubectl describe nodes

      - name: Deploy Helm Chart
        run: |
          cd charts/logzio-telemetry
          helm dependency build
          
          # Set collector-specific resource configuration for testing
          RESOURCE_CONFIG=""
          if [[ "${{ matrix.mode }}" == "standalone" ]]; then
            RESOURCE_CONFIG="--set standaloneCollector.resources.requests.cpu=200m --set standaloneCollector.resources.requests.memory=512Mi --set standaloneCollector.resources.limits.cpu=500m --set standaloneCollector.resources.limits.memory=1Gi"
          else
            RESOURCE_CONFIG="--set daemonsetCollector.resources.requests.cpu=150m --set daemonsetCollector.resources.requests.memory=250Mi --set daemonsetCollector.resources.limits.cpu=300m --set daemonsetCollector.resources.limits.memory=512Mi"
          fi
          
          # Add span metrics resource configuration for testing
          SPM_RESOURCE_CONFIG="--set spanMetricsAgregator.resources.requests.cpu=256m --set spanMetricsAgregator.resources.requests.memory=512Mi --set spanMetricsAgregator.resources.limits.cpu=512m --set spanMetricsAgregator.resources.limits.memory=1Gi"
          
          helm upgrade --install \
          --set traces.enabled=true \
          --set spm.enabled=true \
          --set serviceGraph.enabled=true \
          --set metrics.enabled=true \
          --set global.tolerations[0].key="global-key" \
          --set global.tolerations[0].operator="Equal" \
          --set global.tolerations[0].value="global-value" \
          --set global.tolerations[0].effect="NoSchedule" \
          --set global.logzioTracesToken=${{ secrets.LOGZIO_TRACES_TOKEN }} \
          --set global.logzioSpmToken=${{ secrets.LOGZIO_METRICS_TOKEN }} \
          --set global.logzioMetricsToken=${{ secrets.LOGZIO_METRICS_TOKEN }} \
          --set global.logzioRegion=us \
          --set global.env_id=${{ env.ENV_ID }} \
          --set collector.mode=${{ matrix.mode }} \
          --set spanMetricsAgregator.config.connectors.spanmetrics.histogram.disable=true \
          --set enableMetricsFilter.eks=true \
          --set filters.infrastructure.exclude.namespace="kube-system" \
          --set signalFx.enabled=true \
          --set carbon.enabled=true \
          --set livenessProbe.initialDelaySeconds=10 \
          --set readinessProbe.initialDelaySeconds=10 \
          $RESOURCE_CONFIG \
          $SPM_RESOURCE_CONFIG \
          logzio-k8s-telemetry .

      - name: Debug post-deployment state
        run: |
          echo "=== Helm Release Status ==="
          helm list -A
          helm status logzio-k8s-telemetry
          echo "=== All Resources ==="
          kubectl get all -A
          echo "=== ConfigMaps ==="
          kubectl get configmaps
          echo "=== Secrets ==="
          kubectl get secrets
          echo "=== Events ==="
          kubectl get events --sort-by=.metadata.creationTimestamp
      
      - name: Verify deployment Status
        run: |
          kubectl rollout status deployment/logzio-k8s-telemetry-otel-collector-standalone --timeout=300s
          kubectl rollout status deployment/logzio-k8s-telemetry-otel-collector-spm --timeout=300s
          kubectl describe deployment/logzio-k8s-telemetry-otel-collector-spm
          kubectl describe deployment/logzio-k8s-telemetry-otel-collector-standalone
          kubectl get deployment/logzio-k8s-telemetry-otel-collector-spm -o jsonpath='{.spec.template.spec.tolerations}' | jq -r '.[] | select(.key=="global-key")'
          kubectl get deployment/logzio-k8s-telemetry-otel-collector-standalone -o jsonpath='{.spec.template.spec.tolerations}' | jq -r '.[] | select(.key=="global-key")'

      - name: Debug deployment details
        run: |
          echo "=== Pod Status and Details ==="
          kubectl get pods -o wide
          kubectl describe pods
          echo "=== Pod Logs (Recent) ==="
          for pod in $(kubectl get pods -o jsonpath='{.items[*].metadata.name}'); do
            echo "--- Logs for pod: $pod ---"
            kubectl logs $pod --tail=20 || echo "Failed to get logs for $pod"
          done
          echo "=== Deployment Specifications ==="
          kubectl get deployment logzio-k8s-telemetry-otel-collector-standalone -o yaml || echo "Standalone deployment not found"
          kubectl get deployment logzio-k8s-telemetry-otel-collector-spm -o yaml || echo "SPM deployment not found"
          if [[ "${{ matrix.mode }}" == "daemonset" ]]; then
            echo "=== DaemonSet Specification ==="
            kubectl get daemonset logzio-k8s-telemetry-otel-collector-ds -o yaml || echo "DaemonSet not found"
          fi
      
      - name: Verify Resource Configuration
        run: |
          echo "=== Verifying collector resource configuration ==="
          if [[ "${{ matrix.mode }}" == "standalone" ]]; then
            echo "Checking standalone collector resources..."
            CONTAINER_RESOURCES=$(kubectl get deployment/logzio-k8s-telemetry-otel-collector-standalone -o jsonpath='{.spec.template.spec.containers[0].resources}')
            echo "Container resources: $CONTAINER_RESOURCES"
            # Verify requests and limits are set
            kubectl get deployment/logzio-k8s-telemetry-otel-collector-standalone -o jsonpath='{.spec.template.spec.containers[0].resources.requests.cpu}' | grep -q "200m" || (echo "❌ Expected CPU request 200m not found" && exit 1)
            kubectl get deployment/logzio-k8s-telemetry-otel-collector-standalone -o jsonpath='{.spec.template.spec.containers[0].resources.requests.memory}' | grep -q "512Mi" || (echo "❌ Expected memory request 512Mi not found" && exit 1)
            kubectl get deployment/logzio-k8s-telemetry-otel-collector-standalone -o jsonpath='{.spec.template.spec.containers[0].resources.limits.cpu}' | grep -q "500m" || (echo "❌ Expected CPU limit 500m not found" && exit 1)
            kubectl get deployment/logzio-k8s-telemetry-otel-collector-standalone -o jsonpath='{.spec.template.spec.containers[0].resources.limits.memory}' | grep -q "1Gi" || (echo "❌ Expected memory limit 1Gi not found" && exit 1)
            echo "✅ Standalone collector resources verified successfully"
          else
            echo "Checking daemonset collector resources..."
            CONTAINER_RESOURCES=$(kubectl get daemonset/logzio-k8s-telemetry-otel-collector-ds -o jsonpath='{.spec.template.spec.containers[0].resources}')
            echo "Container resources: $CONTAINER_RESOURCES"
            # Verify requests and limits are set
            kubectl get daemonset/logzio-k8s-telemetry-otel-collector-ds -o jsonpath='{.spec.template.spec.containers[0].resources.requests.cpu}' | grep -q "150m" || (echo "❌ Expected CPU request 150m not found" && exit 1)
            kubectl get daemonset/logzio-k8s-telemetry-otel-collector-ds -o jsonpath='{.spec.template.spec.containers[0].resources.requests.memory}' | grep -q "250Mi" || (echo "❌ Expected memory request 250Mi not found" && exit 1)
            kubectl get daemonset/logzio-k8s-telemetry-otel-collector-ds -o jsonpath='{.spec.template.spec.containers[0].resources.limits.cpu}' | grep -q "300m" || (echo "❌ Expected CPU limit 300m not found" && exit 1)
            kubectl get daemonset/logzio-k8s-telemetry-otel-collector-ds -o jsonpath='{.spec.template.spec.containers[0].resources.limits.memory}' | grep -q "512Mi" || (echo "❌ Expected memory limit 512Mi not found" && exit 1)
            echo "✅ Daemonset collector resources verified successfully"
          fi
          
          # Verify span metrics aggregator resources
          echo "Checking span metrics aggregator resources..."
          SPM_CONTAINER_RESOURCES=$(kubectl get deployment/logzio-k8s-telemetry-otel-collector-spm -o jsonpath='{.spec.template.spec.containers[0].resources}')
          echo "SPM Container resources: $SPM_CONTAINER_RESOURCES"
          kubectl get deployment/logzio-k8s-telemetry-otel-collector-spm -o jsonpath='{.spec.template.spec.containers[0].resources.requests.cpu}' | grep -q "256m" || (echo "❌ Expected SPM CPU request 256m not found" && exit 1)
          kubectl get deployment/logzio-k8s-telemetry-otel-collector-spm -o jsonpath='{.spec.template.spec.containers[0].resources.requests.memory}' | grep -q "512Mi" || (echo "❌ Expected SPM memory request 512Mi not found" && exit 1)
          kubectl get deployment/logzio-k8s-telemetry-otel-collector-spm -o jsonpath='{.spec.template.spec.containers[0].resources.limits.cpu}' | grep -q "512m" || (echo "❌ Expected SPM CPU limit 512m not found" && exit 1)
          kubectl get deployment/logzio-k8s-telemetry-otel-collector-spm -o jsonpath='{.spec.template.spec.containers[0].resources.limits.memory}' | grep -q "1Gi" || (echo "❌ Expected SPM memory limit 1Gi not found" && exit 1)
          echo "✅ Span metrics aggregator resources verified successfully"
      
      - name: Run trace generator
        run: |
          kubectl apply -f tests/resources/tracegen.yaml
          kubectl rollout status deployment/trace-gen --timeout=300s
          echo "=== Trace Generator Status ==="
          kubectl get deployment trace-gen -o wide
          kubectl describe deployment trace-gen
          kubectl logs deployment/trace-gen --tail=10

      - name: Run otel demo
        run: |
          helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts
          helm repo update
          helm install otel-demo -f tests/resources/otel-demo.yaml open-telemetry/opentelemetry-demo --version 0.32.5
          kubectl rollout status deployment/otel-demo-loadgenerator --timeout=300s
          echo "=== OpenTelemetry Demo Status ==="
          kubectl get pods -l app.kubernetes.io/instance=otel-demo
          kubectl describe deployment otel-demo-loadgenerator
          kubectl logs deployment/otel-demo-loadgenerator --tail=10

      - name: sleep for 3 minutes
        run: sleep 180

      - name: Run Go Tests
        env:
          LOGZIO_METRICS_API_KEY: ${{ secrets.LOGZIO_METRICS_API_KEY }}
          LOGZIO_TRACES_API_KEY: ${{ secrets.LOGZIO_TRACES_API_KEY }}
        run: |
          go get go.uber.org/zap
          go test -v ./tests/traces_e2e_test.go ./tests/common.go
          go test -v ./tests/metrics_e2e_test.go ./tests/common.go

      - name: Debug test failure (if any)
        if: failure()
        run: |
          echo "=== Test failure detected - gathering diagnostic information ==="
          echo "=== Final Pod Status ==="
          kubectl get pods -o wide
          echo "=== Final Events ==="
          kubectl get events --sort-by=.metadata.creationTimestamp --no-headers | tail -20
          echo "=== Pod Logs (Extended) ==="
          for pod in $(kubectl get pods -o jsonpath='{.items[*].metadata.name}'); do
            echo "--- Extended logs for pod: $pod ---"
            kubectl logs $pod --tail=50 || echo "Failed to get logs for $pod"
            echo "--- Previous logs for pod: $pod (if exists) ---"
            kubectl logs $pod --previous --tail=20 || echo "No previous logs for $pod"
          done
          echo "=== Resource Usage ==="
          kubectl top nodes || echo "Metrics server not available"
          kubectl top pods || echo "Metrics server not available"
          echo "=== Helm Values Used ==="
          helm get values logzio-k8s-telemetry
          echo "=== Rendered Templates ==="
          cd charts/logzio-telemetry
          # Recreate the same resource configuration used in deployment
          RESOURCE_CONFIG=""
          if [[ "${{ matrix.mode }}" == "standalone" ]]; then
            RESOURCE_CONFIG="--set standaloneCollector.resources.requests.cpu=200m --set standaloneCollector.resources.requests.memory=512Mi --set standaloneCollector.resources.limits.cpu=500m --set standaloneCollector.resources.limits.memory=1Gi"
          else
            RESOURCE_CONFIG="--set daemonsetCollector.resources.requests.cpu=150m --set daemonsetCollector.resources.requests.memory=250Mi --set daemonsetCollector.resources.limits.cpu=300m --set daemonsetCollector.resources.limits.memory=512Mi"
          fi
          SPM_RESOURCE_CONFIG="--set spanMetricsAgregator.resources.requests.cpu=256m --set spanMetricsAgregator.resources.requests.memory=512Mi --set spanMetricsAgregator.resources.limits.cpu=512m --set spanMetricsAgregator.resources.limits.memory=1Gi"
          
          helm template logzio-k8s-telemetry . \
            --set traces.enabled=true \
            --set spm.enabled=true \
            --set serviceGraph.enabled=true \
            --set metrics.enabled=true \
            --set global.tolerations[0].key="global-key" \
            --set global.tolerations[0].operator="Equal" \
            --set global.tolerations[0].value="global-value" \
            --set global.tolerations[0].effect="NoSchedule" \
            --set global.logzioTracesToken=dummy \
            --set global.logzioSpmToken=dummy \
            --set global.logzioMetricsToken=dummy \
            --set global.logzioRegion=us \
            --set global.env_id=${{ env.ENV_ID }} \
            --set collector.mode=${{ matrix.mode }} \
            --set spanMetricsAgregator.config.connectors.spanmetrics.histogram.disable=true \
            --set enableMetricsFilter.eks=true \
            --set filters.infrastructure.exclude.namespace="kube-system" \
            --set signalFx.enabled=true \
            --set carbon.enabled=true \
            $RESOURCE_CONFIG \
            $SPM_RESOURCE_CONFIG || echo "Failed to render templates"
          cd ../..

      - name: Cleanup Environment
        run: |
          helm uninstall logzio-k8s-telemetry

      - name: Delete Kind cluster
        if: always()
        run: kind delete cluster --name kind-${{ github.run_id }}-${{ matrix.mode }}

  test-protocol-receivers:
    name: Test Protocol Receivers
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        mode: ['daemonset', 'standalone']
        protocol: ['signalfx', 'carbon']
    steps:
      - name: Generate random id
        id: random_id
        run: echo "rand=$(echo $RANDOM)" >> $GITHUB_OUTPUT

      - name: Set ENV_ID
        run: echo "ENV_ID=telemetry-test-run-${{ steps.random_id.outputs.rand }}-${{ matrix.mode }}-${{ matrix.protocol }}" >> $GITHUB_ENV

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v6
        with:
          go-version: '1.20'

      - name: Set up Helm
        uses: azure/setup-helm@v4.3.0

      - name: Set up kubectl
        uses: azure/setup-kubectl@v4

      - name: Install Kind
        run: |
          curl -Lo ./kind "https://kind.sigs.k8s.io/dl/v0.26.0/kind-Linux-amd64"
          chmod +x ./kind
          sudo mv ./kind /usr/local/bin/kind

      - name: Create Kind cluster
        run: |
          kind create cluster --name kind-${{ github.run_id }}-${{ matrix.mode }}-${{ matrix.protocol }}
          kubectl cluster-info
          
      - name: Wait for node to be ready
        run: |
          echo "Waiting for nodes to be ready..."
          kubectl wait --for=condition=Ready nodes --all --timeout=300s
          echo "Node status:"
          kubectl get nodes -o wide
          echo "Node details:"
          kubectl describe nodes
          echo "Checking for taints:"
          kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.spec.taints}{"\n"}{end}'
          
      - name: Deploy Helm Chart with Protocol Receiver
        run: |
          cd charts/logzio-telemetry
          helm dependency build
          export RELEASE_NAME="logzio-k8s-telemetry"
          export NAMESPACE="default"
          if [[ "${{ matrix.protocol }}" == "signalfx" ]]; then
            helm upgrade --install -n $NAMESPACE --create-namespace \
            --set metrics.enabled=true \
            --set global.logzioMetricsToken=${{ secrets.LOGZIO_METRICS_TOKEN }} \
            --set global.logzioRegion=us \
            --set global.env_id=${{ env.ENV_ID }} \
            --set collector.mode=${{ matrix.mode }} \
            --set signalFx.enabled=true \
            $RELEASE_NAME .
          elif [[ "${{ matrix.protocol }}" == "carbon" ]]; then
            helm upgrade --install -n $NAMESPACE --create-namespace \
            --set metrics.enabled=true \
            --set global.logzioMetricsToken=${{ secrets.LOGZIO_METRICS_TOKEN }} \
            --set global.logzioRegion=us \
            --set global.env_id=${{ env.ENV_ID }} \
            --set collector.mode=${{ matrix.mode }} \
            --set carbon.enabled=true \
            $RELEASE_NAME .
          fi

      - name: Debug deployment before verification
        run: |
          echo "=== Debugging deployment state ==="
          kubectl get pods -A
          kubectl get nodes -o wide
          echo "=== Checking daemonsets ==="
          kubectl get daemonsets -A
          echo "=== Checking deployments ==="
          kubectl get deployments -A
          if [[ "${{ matrix.mode }}" == "daemonset" ]]; then
            echo "=== Describing daemonset ==="
            kubectl describe daemonset/logzio-k8s-telemetry-otel-collector-ds || echo "Daemonset not found"
            echo "=== Checking for pods with otel-collector labels ==="
            kubectl get pods -l app.kubernetes.io/name=otel-collector
          fi

      - name: Verify deployment Status
        run: |
          echo "=== Checking deployment/daemonset status ==="
          if [[ "${{ matrix.mode }}" == "standalone" ]]; then
            kubectl rollout status deployment/logzio-k8s-telemetry-otel-collector-standalone --timeout=300s
            kubectl get deployment/logzio-k8s-telemetry-otel-collector-standalone -o jsonpath='{.spec.template.spec.tolerations}' | jq -r '.[] | select(.key=="global-key")'
          else
            kubectl rollout status daemonset/logzio-k8s-telemetry-otel-collector-ds --timeout=300s || {
              echo "=== Rollout failed, gathering debug info ==="
              kubectl describe daemonset/logzio-k8s-telemetry-otel-collector-ds
              kubectl get pods -l app.kubernetes.io/name=otel-collector
              kubectl describe pods -l app.kubernetes.io/name=otel-collector
              kubectl logs -l app.kubernetes.io/name=otel-collector --tail=50
              exit 1
            }
            kubectl get daemonset/logzio-k8s-telemetry-otel-collector-ds -o jsonpath='{.spec.template.spec.tolerations}' | jq -r '.[] | select(.key=="global-key")'
          fi

      - name: Verify Default Resource Configuration (Empty)
        run: |
          echo "=== Verifying default (empty) resource configuration ==="
          if [[ "${{ matrix.mode }}" == "standalone" ]]; then
            echo "Checking standalone collector for no resource constraints..."
            CONTAINER_RESOURCES=$(kubectl get deployment/logzio-k8s-telemetry-otel-collector-standalone -o jsonpath='{.spec.template.spec.containers[0].resources}')
            echo "Container resources: $CONTAINER_RESOURCES"
            # Verify that resources is empty or null (default behavior)
            if [[ "$CONTAINER_RESOURCES" == "{}" || "$CONTAINER_RESOURCES" == "" || "$CONTAINER_RESOURCES" == "null" ]]; then
              echo "✅ Standalone collector has no resource constraints (default behavior verified)"
            else
              echo "❌ Expected no resource constraints but found: $CONTAINER_RESOURCES"
              exit 1
            fi
          else
            echo "Checking daemonset collector for no resource constraints..."
            CONTAINER_RESOURCES=$(kubectl get daemonset/logzio-k8s-telemetry-otel-collector-ds -o jsonpath='{.spec.template.spec.containers[0].resources}')
            echo "Container resources: $CONTAINER_RESOURCES"
            # Verify that resources is empty or null (default behavior)
            if [[ "$CONTAINER_RESOURCES" == "{}" || "$CONTAINER_RESOURCES" == "" || "$CONTAINER_RESOURCES" == "null" ]]; then
              echo "✅ Daemonset collector has no resource constraints (default behavior verified)"
            else
              echo "❌ Expected no resource constraints but found: $CONTAINER_RESOURCES"
              exit 1
            fi
          fi

      - name: Run metric generator
        run: |
          DEPLOYMENT_NAME="${{ matrix.protocol }}-gen"
          # Set environment variables for envsubst
          export RELEASE_NAME="logzio-k8s-telemetry"
          export NAMESPACE="default"
          export ENV_ID="${{ env.ENV_ID }}"
          
          # Debug: Show environment variables
          echo "ENV_ID: $ENV_ID"
          echo "RELEASE_NAME: $RELEASE_NAME"
          echo "NAMESPACE: $NAMESPACE"
          
          
          # Substitute ENV_ID in the YAML and apply
          if [[ "${{ matrix.protocol }}" == "signalfx" ]]; then
            envsubst < tests/resources/signalfx-metrics-gen.yaml | kubectl apply -f -
          elif [[ "${{ matrix.protocol }}" == "carbon" ]]; then
            envsubst < tests/resources/carbon-metrics-gen.yaml | kubectl apply -f -            
          fi
          kubectl set env deployment/$DEPLOYMENT_NAME ENV_ID="$ENV_ID"

      - name: Wait for generator deployment
        run: |
          DEPLOYMENT_NAME="${{ matrix.protocol }}-gen"
          
          # Wait for deployment to be available
          kubectl rollout status deployment/$DEPLOYMENT_NAME --timeout=300s
          
          # Check if pods are running
          kubectl get pods -l app=$DEPLOYMENT_NAME
          
          # Show logs
          kubectl logs deployment/$DEPLOYMENT_NAME --tail=10
          
          # If daemonset mode, also check collector status
          if [[ "${{ matrix.mode }}" == "daemonset" ]]; then
            echo "=== Checking collector daemonset status ==="
            kubectl get daemonset -l app.kubernetes.io/name=otel-collector
            kubectl describe daemonset -l app.kubernetes.io/name=otel-collector || echo "No daemonset found"
          fi
 
      - name: Show collector logs
        run: |
          if [[ "${{ matrix.mode }}" == "standalone" ]]; then
            kubectl logs deployment/logzio-k8s-telemetry-otel-collector-standalone --tail=10
          else
            kubectl logs daemonset/logzio-k8s-telemetry-otel-collector-ds --tail=10
          fi
      - name: Sleep for metrics to be processed
        run: sleep 120

      - name: Run Protocol Specific Tests
        env:
          LOGZIO_METRICS_API_KEY: ${{ secrets.LOGZIO_METRICS_API_KEY }}
        run: |
          go get go.uber.org/zap
          if [[ "${{ matrix.protocol }}" == "signalfx" ]]; then
            go test -v ./tests/signalfx_metrics_e2e_test.go ./tests/common.go
          elif [[ "${{ matrix.protocol }}" == "carbon" ]]; then
            go test -v ./tests/carbon_metrics_e2e_test.go ./tests/common.go
          fi

      - name: Cleanup Environment
        run: |
          helm uninstall logzio-k8s-telemetry

      - name: Delete Kind cluster
        if: always()
        run: kind delete cluster --name kind-${{ github.run_id }}-${{ matrix.mode }}-${{ matrix.protocol }}

  filter-test:
    name: Filter Test
    runs-on: ubuntu-latest
    strategy:
      matrix:
        testfile:
          - relabel-advanced.yaml
          - relabel-simple.yaml
        mode: [daemonset, standalone]
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Helm
        uses: azure/setup-helm@v4.3.0

      - name: Set up kubectl
        uses: azure/setup-kubectl@v4

      - name: Install Kind
        run: |
          curl -Lo ./kind "https://kind.sigs.k8s.io/dl/v0.26.0/kind-Linux-amd64"
          chmod +x ./kind
          sudo mv ./kind /usr/local/bin/kind

      - name: Create Kind cluster
        run: |
          kind create cluster --name kind-${{ github.run_id }}-${{ matrix.mode }}
          kubectl cluster-info
      

      - name: Deploy Helm Chart with test values
        run: |
          cd charts/logzio-telemetry
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm dependency build
          helm upgrade --install test . \
            --set collector.mode=${{ matrix.mode }} \
            --set metrics.enabled=true \
            --set applicationMetrics.enabled=true \
            --set global.logzioMetricsToken=dummy \
            -f ../../tests/filters/${{ matrix.testfile }}
      - name: Print ConfigMaps in default namespace
        run: |
          kubectl get configmap -o custom-columns=NAME:.metadata.name
      - name: Wait for ConfigMap
        run: |
          for i in {1..30}; do
            if [[ "${{ matrix.mode }}" == "daemonset" ]]; then
              CONFIGMAP_NAME="test-otel-collector-ds"
            else
              CONFIGMAP_NAME="test-otel-collector-standalone"
            fi
            if kubectl get configmap $CONFIGMAP_NAME; then
              exit 0
            fi
            echo "Waiting for ConfigMap..."
            sleep 5
          done
          echo "ConfigMap not found after waiting."
          kubectl get configmap
          exit 1
          
      - name: Describe config map
        run: |
          if [[ "${{ matrix.mode }}" == "daemonset" ]]; then
            CONFIGMAP_NAME="test-otel-collector-ds"
          else
            CONFIGMAP_NAME="test-otel-collector-standalone"
          fi
          kubectl describe cm $CONFIGMAP_NAME

      - name: Wait for all pods to be Ready
        run: |
          # Wait for all pods to exist
          for i in {1..36}; do
            PODS=$(kubectl get pods --no-headers | wc -l)
            if [[ "$PODS" -gt 0 ]]; then
              break
            fi
            echo "Waiting for pods to be created..."
            sleep 5
          done
          # Now check each pod for true readiness
          for i in {1..36}; do
            NOT_READY=0
            BAD_STATUS=0
            for POD in $(kubectl get pods --no-headers | awk '{print $1}'); do
              PHASE=$(kubectl get pod $POD -o jsonpath='{.status.phase}')
              READY=$(kubectl get pod $POD -o jsonpath='{.status.containerStatuses[*].ready}')
              RESTARTS=$(kubectl get pod $POD -o jsonpath='{.status.containerStatuses[*].restartCount}')
              WAITING_REASON=$(kubectl get pod $POD -o jsonpath='{.status.containerStatuses[0].state.waiting.reason}')
              TERMINATED_REASON=$(kubectl get pod $POD -o jsonpath='{.status.containerStatuses[0].state.terminated.reason}')
              if [[ "$PHASE" != "Running" ]]; then
                echo "Pod $POD is not Running: Phase=$PHASE"
                BAD_STATUS=1
              fi
              if [[ "$READY" != "true" ]]; then
                echo "Pod $POD is not Ready: Ready=$READY"
                NOT_READY=1
              fi
              if [[ "$WAITING_REASON" == "CrashLoopBackOff" || "$WAITING_REASON" == "Error" || "$PHASE" == "Pending" ]]; then
                echo "Pod $POD is unhealthy: WaitingReason=$WAITING_REASON Phase=$PHASE"
                BAD_STATUS=1
              fi
              if [[ "$TERMINATED_REASON" != "" ]]; then
                echo "Pod $POD has terminated: TerminatedReason=$TERMINATED_REASON"
                BAD_STATUS=1
              fi
            done
            if [[ "$NOT_READY" -eq 0 && "$BAD_STATUS" -eq 0 ]]; then
              echo "All pods are healthy and Ready."
              kubectl get pods
              exit 0
            fi
            echo "Waiting for all pods to be healthy and Ready..."
            kubectl get pods
            sleep 5
          done
          echo "ERROR: Not all pods became healthy and Ready in time."
          kubectl get pods
          kubectl describe pods
          exit 1

      - name: Cleanup
        if: always()
        run: |
          helm uninstall test || true
          kind delete cluster --name kind-${{ github.run_id }}-${{ matrix.mode }}

